{
  
    
        "post0": {
            "title": "A fastai Learner from Scratch",
            "content": "This final chapter (other than the conclusion and the online chapters) is going to look a bit different. It contains far more code and far less prose than the previous chapters. We will introduce new Python keywords and libraries without discussing them. This chapter is meant to be the start of a significant research project for you. You see, we are going to implement many of the key pieces of the fastai and PyTorch APIs from scratch, building on nothing other than the components that we developed in &lt;&gt;! The key goal here is to end up with your own Learner class, and some callbacks—enough to be able to train a model on Imagenette, including examples of each of the key techniques we&#39;ve studied. On the way to building Learner, we will create our own version of Module, Parameter, and parallel DataLoader so you have a very good idea of what those PyTorch classes do.&lt;/p&gt; The end-of-chapter questionnaire is particularly important for this chapter. This is where we will be pointing you in the many interesting directions that you could take, using this chapter as your starting point. We suggest that you follow along with this chapter on your computer, and do lots of experiments, web searches, and whatever else you need to understand what&#39;s going on. You&#39;ve built up the skills and expertise to do this in the rest of this book, so we think you are going to do great! . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Let&#39;s begin by gathering (manually) some data. . Data . Have a look at the source to untar_data to see how it works. We&#39;ll use it here to access the 160-pixel version of Imagenette for use in this chapter: . path = untar_data(URLs.IMAGENETTE_160) . To access the image files, we can use get_image_files: . t = get_image_files(path) t[0] . Path(&#39;/home/jhoward/.fastai/data/imagenette2-160/val/n03417042/n03417042_3752.JPEG&#39;) . Or we could do the same thing using just Python&#39;s standard library, with glob: . from glob import glob files = L(glob(f&#39;{path}/**/*.JPEG&#39;, recursive=True)).map(Path) files[0] . Path(&#39;/home/jhoward/.fastai/data/imagenette2-160/val/n03417042/n03417042_3752.JPEG&#39;) . If you look at the source for get_image_files, you&#39;ll see it uses Python&#39;s os.walk; this is a faster and more flexible function than glob, so be sure to try it out. . We can open an image with the Python Imaging Library&#39;s Image class: . im = Image.open(files[0]) im . im_t = tensor(im) im_t.shape . torch.Size([160, 213, 3]) . That&#39;s going to be the basis of our independent variable. For our dependent variable, we can use Path.parent from pathlib. First we&#39;ll need our vocab: . lbls = files.map(Self.parent.name()).unique(); lbls . (#10) [&#39;n03417042&#39;,&#39;n03445777&#39;,&#39;n03888257&#39;,&#39;n03394916&#39;,&#39;n02979186&#39;,&#39;n03000684&#39;,&#39;n03425413&#39;,&#39;n01440764&#39;,&#39;n03028079&#39;,&#39;n02102040&#39;] . ...and the reverse mapping, thanks to L.val2idx: . v2i = lbls.val2idx(); v2i . {&#39;n03417042&#39;: 0, &#39;n03445777&#39;: 1, &#39;n03888257&#39;: 2, &#39;n03394916&#39;: 3, &#39;n02979186&#39;: 4, &#39;n03000684&#39;: 5, &#39;n03425413&#39;: 6, &#39;n01440764&#39;: 7, &#39;n03028079&#39;: 8, &#39;n02102040&#39;: 9} . That&#39;s all the pieces we need to put together our Dataset. . Dataset . A Dataset in PyTorch can be anything that supports indexing (__getitem__) and len: . class Dataset: def __init__(self, fns): self.fns=fns def __len__(self): return len(self.fns) def __getitem__(self, i): im = Image.open(self.fns[i]).resize((64,64)).convert(&#39;RGB&#39;) y = v2i[self.fns[i].parent.name] return tensor(im).float()/255, tensor(y) . We need a list of training and validation filenames to pass to Dataset.__init__: . train_filt = L(o.parent.parent.name==&#39;train&#39; for o in files) train,valid = files[train_filt],files[~train_filt] len(train),len(valid) . (9469, 3925) . Now we can try it out: . train_ds,valid_ds = Dataset(train),Dataset(valid) x,y = train_ds[0] x.shape,y . (torch.Size([64, 64, 3]), tensor(0)) . show_image(x, title=lbls[y]); . As you see, our dataset is returning the independent and dependent variables as a tuple, which is just what we need. We&#39;ll need to be able to collate these into a mini-batch. Generally this is done with torch.stack, which is what we&#39;ll use here: . def collate(idxs, ds): xb,yb = zip(*[ds[i] for i in idxs]) return torch.stack(xb),torch.stack(yb) . Here&#39;s a mini-batch with two items, for testing our collate: . x,y = collate([1,2], train_ds) x.shape,y . (torch.Size([2, 64, 64, 3]), tensor([0, 0])) . Now that we have a dataset and a collation function, we&#39;re ready to create DataLoader. We&#39;ll add two more things here: an optional shuffle for the training set, and a ProcessPoolExecutor to do our preprocessing in parallel. A parallel data loader is very important, because opening and decoding a JPEG image is a slow process. One CPU core is not enough to decode images fast enough to keep a modern GPU busy. Here&#39;s our DataLoader class: . class DataLoader: def __init__(self, ds, bs=128, shuffle=False, n_workers=1): self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers def __len__(self): return (len(self.ds)-1)//self.bs+1 def __iter__(self): idxs = L.range(self.ds) if self.shuffle: idxs = idxs.shuffle() chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)] with ProcessPoolExecutor(self.n_workers) as ex: yield from ex.map(collate, chunks, ds=self.ds) . Let&#39;s try it out with our training and validation datasets: . n_workers = min(16, defaults.cpus) train_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers) valid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers) xb,yb = first(train_dl) xb.shape,yb.shape,len(train_dl) . (torch.Size([128, 64, 64, 3]), torch.Size([128]), 74) . This data loader is not much slower than PyTorch&#39;s, but it&#39;s far simpler. So if you&#39;re debugging a complex data loading process, don&#39;t be afraid to try doing things manually to help you see exactly what&#39;s going on. . For normalization, we&#39;ll need image statistics. Generally it&#39;s fine to calculate these on a single training mini-batch, since precision isn&#39;t needed here: . stats = [xb.mean((0,1,2)),xb.std((0,1,2))] stats . [tensor([0.4544, 0.4453, 0.4141]), tensor([0.2812, 0.2766, 0.2981])] . Our Normalize class just needs to store these stats and apply them (to see why the to_device is needed, try commenting it out, and see what happens later in this notebook): . class Normalize: def __init__(self, stats): self.stats=stats def __call__(self, x): if x.device != self.stats[0].device: self.stats = to_device(self.stats, x.device) return (x-self.stats[0])/self.stats[1] . We always like to test everything we build in a notebook, as soon as we build it: . norm = Normalize(stats) def tfm_x(x): return norm(x).permute((0,3,1,2)) . t = tfm_x(x) t.mean((0,2,3)),t.std((0,2,3)) . (tensor([0.3732, 0.4907, 0.5633]), tensor([1.0212, 1.0311, 1.0131])) . Here tfm_x isn&#39;t just applying Normalize, but is also permuting the axis order from NHWC to NCHW (see &lt;&gt; if you need a reminder of what these acronyms refer to). PIL uses HWC axis order, which we can&#39;t use with PyTorch, hence the need for this permute.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; That&#39;s all we need for the data for our model. So now we need the model itself! . Module and Parameter . To create a model, we&#39;ll need Module. To create Module, we&#39;ll need Parameter, so let&#39;s start there. Recall that in &lt;&gt; we said that the Parameter class &quot;doesn&#39;t actually add any functionality (other than automatically calling requires_grad_ for us). It&#39;s only used as a &quot;marker&quot; to show what to include in parameters.&quot; Here&#39;s a definition which does exactly that:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class Parameter(Tensor): def __new__(self, x): return Tensor._make_subclass(Parameter, x, True) def __init__(self, *args, **kwargs): self.requires_grad_() . The implementation here is a bit awkward: we have to define the special __new__ Python method and use the internal PyTorch method _make_subclass because, as at the time of writing, PyTorch doesn&#39;t otherwise work correctly with this kind of subclassing or provide an officially supported API to do this. This may have been fixed by the time you read this, so look on the book&#39;s website to see if there are updated details. . Our Parameter now behaves just like a tensor, as we wanted: . Parameter(tensor(3.)) . tensor(3., requires_grad=True) . Now that we have this, we can define Module: . class Module: def __init__(self): self.hook,self.params,self.children,self._training = None,[],[],False def register_parameters(self, *ps): self.params += ps def register_modules (self, *ms): self.children += ms @property def training(self): return self._training @training.setter def training(self,v): self._training = v for m in self.children: m.training=v def parameters(self): return self.params + sum([m.parameters() for m in self.children], []) def __setattr__(self,k,v): super().__setattr__(k,v) if isinstance(v,Parameter): self.register_parameters(v) if isinstance(v,Module): self.register_modules(v) def __call__(self, *args, **kwargs): res = self.forward(*args, **kwargs) if self.hook is not None: self.hook(res, args) return res def cuda(self): for p in self.parameters(): p.data = p.data.cuda() . The key functionality is in the definition of parameters: . self.params + sum([m.parameters() for m in self.children], []) . This means that we can ask any Module for its parameters, and it will return them, including for all its child modules (recursively). But how does it know what its parameters are? It&#39;s thanks to implementing Python&#39;s special __setattr__ method, which is called for us any time Python sets an attribute on a class. Our implementation includes this line: . if isinstance(v,Parameter): self.register_parameters(v) . As you see, this is where we use our new Parameter class as a &quot;marker&quot;—anything of this class is added to our params. . Python&#39;s __call__ allows us to define what happens when our object is treated as a function; we just call forward (which doesn&#39;t exist here, so it&#39;ll need to be added by subclasses). Before we do, we&#39;ll call a hook, if it&#39;s defined. Now you can see that PyTorch hooks aren&#39;t doing anything fancy at all—they&#39;re just calling any hooks have been registered. . Other than these pieces of functionality, our Module also provides cuda and training attributes, which we&#39;ll use shortly. . Now we can create our first Module, which is ConvLayer: . class ConvLayer(Module): def __init__(self, ni, nf, stride=1, bias=True, act=True): super().__init__() self.w = Parameter(torch.zeros(nf,ni,3,3)) self.b = Parameter(torch.zeros(nf)) if bias else None self.act,self.stride = act,stride init = nn.init.kaiming_normal_ if act else nn.init.xavier_normal_ init(self.w) def forward(self, x): x = F.conv2d(x, self.w, self.b, stride=self.stride, padding=1) if self.act: x = F.relu(x) return x . We&#39;re not implementing F.conv2d from scratch, since you should have already done that (using unfold) in the questionnaire in &lt;&gt;. Instead, we&#39;re just creating a small class that wraps it up along with bias and weight initialization. Let&#39;s check that it works correctly with Module.parameters:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; l = ConvLayer(3, 4) len(l.parameters()) . 2 . And that we can call it (which will result in forward being called): . xbt = tfm_x(xb) r = l(xbt) r.shape . torch.Size([128, 4, 64, 64]) . In the same way, we can implement Linear: . class Linear(Module): def __init__(self, ni, nf): super().__init__() self.w = Parameter(torch.zeros(nf,ni)) self.b = Parameter(torch.zeros(nf)) nn.init.xavier_normal_(self.w) def forward(self, x): return x@self.w.t() + self.b . and test if it works: . l = Linear(4,2) r = l(torch.ones(3,4)) r.shape . torch.Size([3, 2]) . Let&#39;s also create a testing module to check that if we include multiple parameters as attributes, they are all correctly registered: . class T(Module): def __init__(self): super().__init__() self.c,self.l = ConvLayer(3,4),Linear(4,2) . Since we have a conv layer and a linear layer, each of which has weights and biases, we&#39;d expect four parameters in total: . t = T() len(t.parameters()) . 4 . We should also find that calling cuda on this class puts all these parameters on the GPU: . t.cuda() t.l.w.device . device(type=&#39;cuda&#39;, index=5) . We can now use those pieces to create a CNN. . Simple CNN . As we&#39;ve seen, a Sequential class makes many architectures easier to implement, so let&#39;s make one: . class Sequential(Module): def __init__(self, *layers): super().__init__() self.layers = layers self.register_modules(*layers) def forward(self, x): for l in self.layers: x = l(x) return x . The forward method here just calls each layer in turn. Note that we have to use the register_modules method we defined in Module, since otherwise the contents of layers won&#39;t appear in parameters. . . Important: All The Code is Here: Remember that we&#8217;re not using any PyTorch functionality for modules here; we&#8217;re defining everything ourselves. So if you&#8217;re not sure what register_modules does, or why it&#8217;s needed, have another look at our code for Module to see what we wrote! . We can create a simplified AdaptivePool that only handles pooling to a 1×1 output, and flattens it as well, by just using mean: . class AdaptivePool(Module): def forward(self, x): return x.mean((2,3)) . That&#39;s enough for us to create a CNN! . def simple_cnn(): return Sequential( ConvLayer(3 ,16 ,stride=2), #32 ConvLayer(16,32 ,stride=2), #16 ConvLayer(32,64 ,stride=2), # 8 ConvLayer(64,128,stride=2), # 4 AdaptivePool(), Linear(128, 10) ) . Let&#39;s see if our parameters are all being registered correctly: . m = simple_cnn() len(m.parameters()) . 10 . Now we can try adding a hook. Note that we&#39;ve only left room for one hook in Module; you could make it a list, or use something like Pipeline to run a few as a single function: . def print_stats(outp, inp): print (outp.mean().item(),outp.std().item()) for i in range(4): m.layers[i].hook = print_stats r = m(xbt) r.shape . 0.5239089727401733 0.8776043057441711 0.43470510840415955 0.8347987532615662 0.4357188045978546 0.7621666193008423 0.46562111377716064 0.7416611313819885 . torch.Size([128, 10]) . We have data and model. Now we need a loss function. . Loss . We&#39;ve already seen how to define &quot;negative log likelihood&quot;: . def nll(input, target): return -input[range(target.shape[0]), target].mean() . Well actually, there&#39;s no log here, since we&#39;re using the same definition as PyTorch. That means we need to put the log together with softmax: . def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log() sm = log_softmax(r); sm[0][0] . tensor(-1.2790, grad_fn=&lt;SelectBackward&gt;) . Combining these gives us our cross-entropy loss: . loss = nll(sm, yb) loss . tensor(2.5666, grad_fn=&lt;NegBackward&gt;) . Note that the formula: . $$ log left ( frac{a}{b} right ) = log(a) - log(b)$$ . gives a simplification when we compute the log softmax, which was previously defined as (x.exp()/(x.exp().sum(-1))).log(): . def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log() sm = log_softmax(r); sm[0][0] . tensor(-1.2790, grad_fn=&lt;SelectBackward&gt;) . Then, there is a more stable way to compute the log of the sum of exponentials, called the LogSumExp trick. The idea is to use the following formula: . $$ log left ( sum_{j=1}^{n} e^{x_{j}} right ) = log left ( e^{a} sum_{j=1}^{n} e^{x_{j}-a} right ) = a + log left ( sum_{j=1}^{n} e^{x_{j}-a} right )$$ . where $a$ is the maximum of $x_{j}$. . Here&#39;s the same thing in code: . x = torch.rand(5) a = x.max() x.exp().sum().log() == a + (x-a).exp().sum().log() . tensor(True) . We&#39;ll put that into a function: . def logsumexp(x): m = x.max(-1)[0] return m + (x-m[:,None]).exp().sum(-1).log() logsumexp(r)[0] . tensor(3.9784, grad_fn=&lt;SelectBackward&gt;) . so we can use it for our log_softmax function: . def log_softmax(x): return x - x.logsumexp(-1,keepdim=True) . Which gives the same result as before: . sm = log_softmax(r); sm[0][0] . tensor(-1.2790, grad_fn=&lt;SelectBackward&gt;) . We can use these to create cross_entropy: . def cross_entropy(preds, yb): return nll(log_softmax(preds), yb).mean() . Let&#39;s now combine all those pieces together to create a Learner. . Learner . We have data, a model, and a loss function; we only need one more thing we can fit a model, and that&#39;s an optimizer! Here&#39;s SGD: . class SGD: def __init__(self, params, lr, wd=0.): store_attr() def step(self): for p in self.params: p.data -= (p.grad.data + p.data*self.wd) * self.lr p.grad.data.zero_() . As we&#39;ve seen in this book, life is easier with a Learner. The Learner class needs to know our training and validation sets, which means we need DataLoaders to store them. We don&#39;t need any other functionality, just a place to store them and access them: . class DataLoaders: def __init__(self, *dls): self.train,self.valid = dls dls = DataLoaders(train_dl,valid_dl) . Now we&#39;re ready to create our Learner class: . class Learner: def __init__(self, model, dls, loss_func, lr, cbs, opt_func=SGD): store_attr() for cb in cbs: cb.learner = self def one_batch(self): self(&#39;before_batch&#39;) xb,yb = self.batch self.preds = self.model(xb) self.loss = self.loss_func(self.preds, yb) if self.model.training: self.loss.backward() self.opt.step() self(&#39;after_batch&#39;) def one_epoch(self, train): self.model.training = train self(&#39;before_epoch&#39;) dl = self.dls.train if train else self.dls.valid for self.num,self.batch in enumerate(progress_bar(dl, leave=False)): self.one_batch() self(&#39;after_epoch&#39;) def fit(self, n_epochs): self(&#39;before_fit&#39;) self.opt = self.opt_func(self.model.parameters(), self.lr) self.n_epochs = n_epochs try: for self.epoch in range(n_epochs): self.one_epoch(True) self.one_epoch(False) except CancelFitException: pass self(&#39;after_fit&#39;) def __call__(self,name): for cb in self.cbs: getattr(cb,name,noop)() . This is the largest class we&#39;ve created in the book, but each method is quite small, so by looking at each in turn you should be able to follow what&#39;s going on. . The main method we&#39;ll be calling is fit. This loops with: . for self.epoch in range(n_epochs) . and at each epoch calls self.one_epoch for each of train=True and then train=False. Then self.one_epoch calls self.one_batch for each batch in dls.train or dls.valid, as appropriate (after wrapping the DataLoader in fastprogress.progress_bar. Finally, self.one_batch follows the usual set of steps to fit one mini-batch that we&#39;ve seen throughout this book. . Before and after each step, Learner calls self, which calls __call__ (which is standard Python functionality). __call__ uses getattr(cb,name) on each callback in self.cbs, which is a Python built-in function that returns the attribute (a method, in this case) with the requested name. So, for instance, self(&#39;before_fit&#39;) will call cb.before_fit() for each callback where that method is defined. . As you can see, Learner is really just using our standard training loop, except that it&#39;s also calling callbacks at appropriate times. So let&#39;s define some callbacks! . Callbacks . In Learner.__init__ we have: . for cb in cbs: cb.learner = self . In other words, every callback knows what learner it is used in. This is critical, since otherwise a callback can&#39;t get information from the learner, or change things in the learner. Because getting information from the learner is so common, we make that easier by defining Callback as a subclass of GetAttr, with a default attribute of learner: . class Callback(GetAttr): _default=&#39;learner&#39; . GetAttr is a fastai class that implements Python&#39;s standard __getattr__ and __dir__ methods for you, such that any time you try to access an attribute that doesn&#39;t exist, it passes the request along to whatever you have defined as _default. . For instance, we want to move all model parameters to the GPU automatically at the start of fit. We could do this by defining before_fit as self.learner.model.cuda(); however, because learner is the default attribute, and we have SetupLearnerCB inherit from Callback (which inherits from GetAttr), we can remove the .learner and just call self.model.cuda(): . class SetupLearnerCB(Callback): def before_batch(self): xb,yb = to_device(self.batch) self.learner.batch = tfm_x(xb),yb def before_fit(self): self.model.cuda() . In SetupLearnerCB we also move each mini-batch to the GPU, by calling to_device(self.batch) (we could also have used the longer to_device(self.learner.batch). Note however that in the line self.learner.batch = tfm_x(xb),yb we can&#39;t remove .learner, because here we&#39;re setting the attribute, not getting it. . Before we try our Learner out, let&#39;s create a callback to track and print progress. Otherwise we won&#39;t really know if it&#39;s working properly: . class TrackResults(Callback): def before_epoch(self): self.accs,self.losses,self.ns = [],[],[] def after_epoch(self): n = sum(self.ns) print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n) def after_batch(self): xb,yb = self.batch acc = (self.preds.argmax(dim=1)==yb).float().sum() self.accs.append(acc) n = len(xb) self.losses.append(self.loss*n) self.ns.append(n) . Now we&#39;re ready to use our Learner for the first time! . cbs = [SetupLearnerCB(),TrackResults()] learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs) learn.fit(1) . 0 True 2.1275552130636814 0.2314922378287042 . 0 False 1.9942575636942674 0.2991082802547771 . It&#39;s quite amazing to realize that we can implement all the key ideas from fastai&#39;s Learner in so little code! Let&#39;s now add some learning rate scheduling. . Scheduling the Learning Rate . If we&#39;re going to get good results, we&#39;ll want an LR finder and 1cycle training. These are both annealing callbacks—that is, they are gradually changing hyperparameters as we train. Here&#39;s LRFinder: . class LRFinder(Callback): def before_fit(self): self.losses,self.lrs = [],[] self.learner.lr = 1e-6 def before_batch(self): if not self.model.training: return self.opt.lr *= 1.2 def after_batch(self): if not self.model.training: return if self.opt.lr&gt;10 or torch.isnan(self.loss): raise CancelFitException self.losses.append(self.loss.item()) self.lrs.append(self.opt.lr) . This shows how we&#39;re using CancelFitException, which is itself an empty class, only used to signify the type of exception. You can see in Learner that this exception is caught. (You should add and test CancelBatchException, CancelEpochException, etc. yourself.) Let&#39;s try it out, by adding it to our list of callbacks: . lrfind = LRFinder() learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind]) learn.fit(2) . 0 True 2.6336045582954903 0.11014890695955222 . 0 False 2.230653363853503 0.18318471337579617 . &lt;progress value=&#39;12&#39; class=&#39;&#39; max=&#39;74&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 16.22% [12/74 00:02&lt;00:12] And take a look at the results: . plt.plot(lrfind.lrs[:-2],lrfind.losses[:-2]) plt.xscale(&#39;log&#39;) . Now we can define our OneCycle training callback: . class OneCycle(Callback): def __init__(self, base_lr): self.base_lr = base_lr def before_fit(self): self.lrs = [] def before_batch(self): if not self.model.training: return n = len(self.dls.train) bn = self.epoch*n + self.num mn = self.n_epochs*n pct = bn/mn pct_start,div_start = 0.25,10 if pct&lt;pct_start: pct /= pct_start lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr else: pct = (pct-pct_start)/(1-pct_start) lr = (1-pct)*self.base_lr self.opt.lr = lr self.lrs.append(lr) . We&#39;ll try an LR of 0.1: . onecyc = OneCycle(0.1) learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[onecyc]) . Let&#39;s fit for a while and see how it looks (we won&#39;t show all the output in the book—try it in the notebook to see the results): . learn.fit(8) . Finally, we&#39;ll check that the learning rate followed the schedule we defined (as you see, we&#39;re not using cosine annealing here): . plt.plot(onecyc.lrs); . Conclusion . We have explored the key concepts of the fastai library are implemented by re-implementing them in this chapter. Since it&#39;s mostly full of code, you should definitely try to experiment with it by looking at the corresponding notebook on the book&#39;s website. Now that you know how it&#39;s built, as a next step be sure to check out the intermediate and advanced tutorials in the fastai documentation to learn how to customize every bit of the library. . Questionnaire . . Tip: Experiments: For the questions here that ask you to explain what some function or class is, you should also complete your own code experiments. . What is glob? | How do you open an image with the Python imaging library? | What does L.map do? | What does Self do? | What is L.val2idx? | What methods do you need to implement to create your own Dataset? | Why do we call convert when we open an image from Imagenette? | What does ~ do? How is it useful for splitting training and validation sets? | Does ~ work with the L or Tensor classes? What about NumPy arrays, Python lists, or pandas DataFrames? | What is ProcessPoolExecutor? | How does L.range(self.ds) work? | What is __iter__? | What is first? | What is permute? Why is it needed? | What is a recursive function? How does it help us define the parameters method? | Write a recursive function that returns the first 20 items of the Fibonacci sequence. | What is super? | Why do subclasses of Module need to override forward instead of defining __call__? | In ConvLayer, why does init depend on act? | Why does Sequential need to call register_modules? | Write a hook that prints the shape of every layer&#39;s activations. | What is &quot;LogSumExp&quot;? | Why is log_softmax useful? | What is GetAttr? How is it helpful for callbacks? | Reimplement one of the callbacks in this chapter without inheriting from Callback or GetAttr. | What does Learner.__call__ do? | What is getattr? (Note the case difference to GetAttr!) | Why is there a try block in fit? | Why do we check for model.training in one_batch? | What is store_attr? | What is the purpose of TrackResults.before_epoch? | What does model.cuda do? How does it work? | Why do we need to check model.training in LRFinder and OneCycle? | Use cosine annealing in OneCycle. | Further Research . Write resnet18 from scratch (refer to &lt;&gt; as needed), and train it with the Learner in this chapter.&lt;/li&gt; Implement a batchnorm layer from scratch and use it in your resnet18. | Write a Mixup callback for use in this chapter. | Add momentum to SGD. | Pick a few features that you&#39;re interested in from fastai (or any other library) and implement them in this chapter. | Pick a research paper that&#39;s not yet implemented in fastai or PyTorch and implement it in this chapter. Port it over to fastai. | Submit a pull request to fastai, or create your own extension module and release it. | Hint: you may find it helpful to use nbdev to create and deploy your package. | . | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; | .",
            "url": "https://pranath.github.io/fastbook/2021/02/19/19_learner.html",
            "relUrl": "/2021/02/19/19_learner.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "CNN Interpretation with CAM",
            "content": "[[chapter_cam]] Now that we know how to build up pretty much anything from scratch, let&#39;s use that knowledge to create entirely new (and very useful!) functionality: the class activation map. It gives us some insight into why a CNN made the predictions it did. . In the process, we&#39;ll learn about one handy feature of PyTorch we haven&#39;t seen before, the hook, and we&#39;ll apply many of the concepts introduced in the rest of the book. If you want to really test out your understanding of the material in this book, after you&#39;ve finished this chapter, try putting it aside and recreating the ideas here yourself from scratch (no peeking!). . CAM and Hooks . The class activation map (CAM) was introduced by Bolei Zhou et al. in &quot;Learning Deep Features for Discriminative Localization&quot;. It uses the output of the last convolutional layer (just before the average pooling layer) together with the predictions to give us a heatmap visualization of why the model made its decision. This is a useful tool for interpretation. . More precisely, at each position of our final convolutional layer, we have as many filters as in the last linear layer. We can therefore compute the dot product of those activations with the final weights to get, for each location on our feature map, the score of the feature that was used to make a decision. . We&#39;re going to need a way to get access to the activations inside the model while it&#39;s training. In PyTorch this can be done with a hook. Hooks are PyTorch&#39;s equivalent of fastai&#39;s callbacks. However, rather than allowing you to inject code into the training loop like a fastai Learner callback, hooks allow you to inject code into the forward and backward calculations themselves. We can attach a hook to any layer of the model, and it will be executed when we compute the outputs (forward hook) or during backpropagation (backward hook). A forward hook is a function that takes three things—a module, its input, and its output—and it can perform any behavior you want. (fastai also provides a handy HookCallback that we won&#39;t cover here, but take a look at the fastai docs; it makes working with hooks a little easier.) . To illustrate, we&#39;ll use the same cats and dogs model we trained in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=21, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.145994 | 0.019272 | 0.006089 | 00:14 | . epoch train_loss valid_loss error_rate time . 0 | 0.053405 | 0.052540 | 0.010825 | 00:19 | . To start, we&#39;ll grab a cat picture and a batch of data: . img = PILImage.create(image_cat()) x, = first(dls.test_dl([img])) . For CAM we want to store the activations of the last convolutional layer. We put our hook function in a class so it has a state that we can access later, and just store a copy of the output: . class Hook(): def hook_func(self, m, i, o): self.stored = o.detach().clone() . We can then instantiate a Hook and attach it to the layer we want, which is the last layer of the CNN body: . hook_output = Hook() hook = learn.model[0].register_forward_hook(hook_output.hook_func) . Now we can grab a batch and feed it through our model: . with torch.no_grad(): output = learn.model.eval()(x) . And we can access our stored activations: . act = hook_output.stored[0] . Let&#39;s also double-check our predictions: . F.softmax(output, dim=-1) . tensor([[0.0010, 0.9990]], device=&#39;cuda:0&#39;) . We know 0 (for False) is &quot;dog,&quot; because the classes are automatically sorted in fastai, bu we can still double-check by looking at dls.vocab: . dls.vocab . (#2) [False,True] . So, our model is very confident this was a picture of a cat. . To do the dot product of our weight matrix (2 by number of activations) with the activations (batch size by activations by rows by cols), we use a custom einsum: . x.shape . torch.Size([1, 3, 224, 224]) . cam_map = torch.einsum(&#39;ck,kij-&gt;cij&#39;, learn.model[1][-1].weight, act) cam_map.shape . torch.Size([2, 7, 7]) . For each image in our batch, and for each class, we get a 7×7 feature map that tells us where the activations were higher and where they were lower. This will let us see which areas of the pictures influenced the model&#39;s decision. . For instance, we can find out which areas made the model decide this animal was a cat (note that we need to decode the input x since it&#39;s been normalized by the DataLoader, and we need to cast to TensorImage since at the time this book is written PyTorch does not maintain types when indexing—this may be fixed by the time you are reading this): . x_dec = TensorImage(dls.train.decode((x,))[0][0]) _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . The areas in bright yellow correspond to high activations and the areas in purple to low activations. In this case, we can see the head and the front paw were the two main areas that made the model decide it was a picture of a cat. . Once you&#39;re done with your hook, you should remove it as otherwise it might leak some memory: . hook.remove() . That&#39;s why it&#39;s usually a good idea to have the Hook class be a context manager, registering the hook when you enter it and removing it when you exit. A context manager is a Python construct that calls __enter__ when the object is created in a with clause, and __exit__ at the end of the with clause. For instance, this is how Python handles the with open(...) as f: construct that you&#39;ll often see for opening files without requiring an explicit close(f) at the end. If we define Hook as follows: . class Hook(): def __init__(self, m): self.hook = m.register_forward_hook(self.hook_func) def hook_func(self, m, i, o): self.stored = o.detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . we can safely use it this way: . with Hook(learn.model[0]) as hook: with torch.no_grad(): output = learn.model.eval()(x.cuda()) act = hook.stored . fastai provides this Hook class for you, as well as some other handy classes to make working with hooks easier. . This method is useful, but only works for the last layer. Gradient CAM is a variant that addresses this problem. . Gradient CAM . The method we just saw only lets us compute a heatmap with the last activations, since once we have our features, we have to multiply them by the last weight matrix. This won&#39;t work for inner layers in the network. A variant introduced in the paper &quot;Grad-CAM: Why Did You Say That? Visual Explanations from Deep Networks via Gradient-based Localization&quot; in 2016 uses the gradients of the final activation for the desired class. If you remember a little bit about the backward pass, the gradients of the output of the last layer with respect to the input of that layer are equal to the layer weights, since it is a linear layer. . With deeper layers, we still want the gradients, but they won&#39;t just be equal to the weights anymore. We have to calculate them. The gradients of every layer are calculated for us by PyTorch during the backward pass, but they&#39;re not stored (except for tensors where requires_grad is True). We can, however, register a hook on the backward pass, which PyTorch will give the gradients to as a parameter, so we can store them there. For this we will use a HookBwd class that works like Hook, but intercepts and stores gradients instead of activations: . class HookBwd(): def __init__(self, m): self.hook = m.register_backward_hook(self.hook_func) def hook_func(self, m, gi, go): self.stored = go[0].detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . Then for the class index 1 (for True, which is &quot;cat&quot;) we intercept the features of the last convolutional layer as before, and compute the gradients of the output activations of our class. We can&#39;t just call output.backward(), because gradients only make sense with respect to a scalar (which is normally our loss) and output is a rank-2 tensor. But if we pick a single image (we&#39;ll use 0) and a single class (we&#39;ll use 1), then we can calculate the gradients of any weight or activation we like, with respect to that single value, using output[0,cls].backward(). Our hook intercepts the gradients that we&#39;ll use as weights: . cls = 1 with HookBwd(learn.model[0]) as hookg: with Hook(learn.model[0]) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored . The weights for our Grad-CAM are given by the average of our gradients across the feature map. Then it&#39;s exactly the same as before: . w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) . _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . The novelty with Grad-CAM is that we can use it on any layer. For example, here we use it on the output of the second-to-last ResNet group: . with HookBwd(learn.model[0][-2]) as hookg: with Hook(learn.model[0][-2]) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored . w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) . And we can now view the activation map for this layer: . _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . Conclusion . Model interpretation is an area of active research, and we just scraped the surface of what is possible in this brief chapter. Class activation maps give us insight into why a model predicted a certain result by showing the areas of the images that were most responsible for a given prediction. This can help us analyze false positives and figure out what kind of data is missing in our training to avoid them. . Questionnaire . What is a &quot;hook&quot; in PyTorch? | Which layer does CAM use the outputs of? | Why does CAM require a hook? | Look at the source code of the ActivationStats class and see how it uses hooks. | Write a hook that stores the activations of a given layer in a model (without peeking, if possible). | Why do we call eval before getting the activations? Why do we use no_grad? | Use torch.einsum to compute the &quot;dog&quot; or &quot;cat&quot; score of each of the locations in the last activation of the body of the model. | How do you check which order the categories are in (i.e., the correspondence of index-&gt;category)? | Why are we using decode when displaying the input image? | What is a &quot;context manager&quot;? What special methods need to be defined to create one? | Why can&#39;t we use plain CAM for the inner layers of a network? | Why do we need to register a hook on the backward pass in order to do Grad-CAM? | Why can&#39;t we call output.backward() when output is a rank-2 tensor of output activations per image per class? | Further Research . Try removing keepdim and see what happens. Look up this parameter in the PyTorch docs. Why do we need it in this notebook? | Create a notebook like this one, but for NLP, and use it to find which words in a movie review are most significant in assessing the sentiment of a particular movie review. | &lt;/div&gt; .",
            "url": "https://pranath.github.io/fastbook/2021/02/19/18_CAM.html",
            "relUrl": "/2021/02/19/18_CAM.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "A Neural Net from the Foundations",
            "content": "[[chapter_foundations]] This chapter begins a journey where we will dig deep into the internals of the models we used in the previous chapters. We will be covering many of the same things we&#39;ve seen before, but this time around we&#39;ll be looking much more closely at the implementation details, and much less closely at the practical issues of how and why things are as they are. . We will build everything from scratch, only using basic indexing into a tensor. We&#39;ll write a neural net from the ground up, then implement backpropagation manually, so we know exactly what&#39;s happening in PyTorch when we call loss.backward. We&#39;ll also see how to extend PyTorch with custom autograd functions that allow us to specify our own forward and backward computations. . Building a Neural Net Layer from Scratch . Let&#39;s start by refreshing our understanding of how matrix multiplication is used in a basic neural network. Since we&#39;re building everything up from scratch, we&#39;ll use nothing but plain Python initially (except for indexing into PyTorch tensors), and then replace the plain Python with PyTorch functionality once we&#39;ve seen how to create it. . Modeling a Neuron . A neuron receives a given number of inputs and has an internal weight for each of them. It sums those weighted inputs to produce an output and adds an inner bias. In math, this can be written as: . $$ out = sum_{i=1}^{n} x_{i} w_{i} + b$$ . if we name our inputs $(x_{1}, dots,x_{n})$, our weights $(w_{1}, dots,w_{n})$, and our bias $b$. In code this translates into: . output = sum([x*w for x,w in zip(inputs,weights)]) + bias . This output is then fed into a nonlinear function called an activation function before being sent to another neuron. In deep learning the most common of these is the rectified Linear unit, or ReLU, which, as we&#39;ve seen, is a fancy way of saying: . def relu(x): return x if x &gt;= 0 else 0 . A deep learning model is then built by stacking a lot of those neurons in successive layers. We create a first layer with a certain number of neurons (known as hidden size) and link all the inputs to each of those neurons. Such a layer is often called a fully connected layer or a dense layer (for densely connected), or a linear layer. . It requires to compute, for each input in our batch and each neuron with a give weight, the dot product: . sum([x*w for x,w in zip(input,weight)]) . If you have done a little bit of linear algebra, you may remember that having a lot of those dot products happens when you do a matrix multiplication. More precisely, if our inputs are in a matrix x with a size of batch_size by n_inputs, and if we have grouped the weights of our neurons in a matrix w of size n_neurons by n_inputs (each neuron must have the same number of weights as it has inputs) and all the biases in a vector b of size n_neurons, then the output of this fully connected layer is: . y = x @ w.t() + b . where @ represents the matrix product and w.t() is the transpose matrix of w. The output y is then of size batch_size by n_neurons, and in position (i,j) we have (for the mathy folks out there): . $$y_{i,j} = sum_{k=1}^{n} x_{i,k} w_{k,j} + b_{j}$$ . Or in code: . y[i,j] = sum([a * b for a,b in zip(x[i,:],w[j,:])]) + b[j] . The transpose is necessary because in the mathematical definition of the matrix product m @ n, the coefficient (i,j) is: . sum([a * b for a,b in zip(m[i,:],n[:,j])]) . So the very basic operation we need is a matrix multiplication, as it&#39;s what is hidden in the core of a neural net. . Matrix Multiplication from Scratch . Let&#39;s write a function that computes the matrix product of two tensors, before we allow ourselves to use the PyTorch version of it. We will only use the indexing in PyTorch tensors: . import torch from torch import tensor . We&#39;ll need three nested for loops: one for the row indices, one for the column indices, and one for the inner sum. ac and ar stand for number of columns of a and number of rows of a, respectively (the same convention is followed for b), and we make sure calculating the matrix product is possible by checking that a has as many columns as b has rows: . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): c[i,j] += a[i,k] * b[k,j] return c . To test this out, we&#39;ll pretend (using random matrices) that we&#39;re working with a small batch of 5 MNIST images, flattened into 28×28 vectors, with linear model to turn them into 10 activations: . m1 = torch.randn(5,28*28) m2 = torch.randn(784,10) . Let&#39;s time our function, using the Jupyter &quot;magic&quot; command %time: . %time t1=matmul(m1, m2) . CPU times: user 1.15 s, sys: 4.09 ms, total: 1.15 s Wall time: 1.15 s . And see how that compares to PyTorch&#39;s built-in @: . %timeit -n 20 t2=m1@m2 . 14 µs ± 8.95 µs per loop (mean ± std. dev. of 7 runs, 20 loops each) . As we can see, in Python three nested loops is a very bad idea! Python is a slow language, and this isn&#39;t going to be very efficient. We see here that PyTorch is around 100,000 times faster than Python—and that&#39;s before we even start using the GPU! . Where does this difference come from? PyTorch didn&#39;t write its matrix multiplication in Python, but rather in C++ to make it fast. In general, whenever we do computations on tensors we will need to vectorize them so that we can take advantage of the speed of PyTorch, usually by using two techniques: elementwise arithmetic and broadcasting. . Elementwise Arithmetic . All the basic operators (+, -, *, /, &gt;, &lt;, ==) can be applied elementwise. That means if we write a+b for two tensors a and b that have the same shape, we will get a tensor composed of the sums the elements of a and b: . a = tensor([10., 6, -4]) b = tensor([2., 8, 7]) a + b . tensor([12., 14., 3.]) . The Booleans operators will return an array of Booleans: . a &lt; b . tensor([False, True, True]) . If we want to know if every element of a is less than the corresponding element in b, or if two tensors are equal, we need to combine those elementwise operations with torch.all: . (a &lt; b).all(), (a==b).all() . (tensor(False), tensor(False)) . Reduction operations like all(), sum() and mean() return tensors with only one element, called rank-0 tensors. If you want to convert this to a plain Python Boolean or number, you need to call .item(): . (a + b).mean().item() . 9.666666984558105 . The elementwise operations work on tensors of any rank, as long as they have the same shape: . m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]) m*m . tensor([[ 1., 4., 9.], [16., 25., 36.], [49., 64., 81.]]) . However you can&#39;t perform elementwise operations on tensors that don&#39;t have the same shape (unless they are broadcastable, as discussed in the next section): . n = tensor([[1., 2, 3], [4,5,6]]) m*n . RuntimeError Traceback (most recent call last) &lt;ipython-input-12-add73c4f74e0&gt; in &lt;module&gt; 1 n = tensor([[1., 2, 3], [4,5,6]]) -&gt; 2 m*n RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0 . With elementwise arithmetic, we can remove one of our three nested loops: we can multiply the tensors that correspond to the i-th row of a and the j-th column of b before summing all the elements, which will speed things up because the inner loop will now be executed by PyTorch at C speed. . To access one column or row, we can simply write a[i,:] or b[:,j]. The : means take everything in that dimension. We could restrict this and take only a slice of that particular dimension by passing a range, like 1:5, instead of just :. In that case, we would take the elements in columns or rows 1 to 4 (the second number is noninclusive). . One simplification is that we can always omit a trailing colon, so a[i,:] can be abbreviated to a[i]. With all of that in mind, we can write a new version of our matrix multiplication: . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum() return c . %timeit -n 20 t3 = matmul(m1,m2) . 1.7 ms ± 88.1 µs per loop (mean ± std. dev. of 7 runs, 20 loops each) . We&#39;re already ~700 times faster, just by removing that inner for loop! And that&#39;s just the beginning—with broadcasting we can remove another loop and get an even more important speed up. . Broadcasting . As we discussed in &lt;&gt;, broadcasting is a term introduced by the NumPy library that describes how tensors of different ranks are treated during arithmetic operations. For instance, it&#39;s obvious there is no way to add a 3×3 matrix with a 4×5 matrix, but what if we want to add one scalar (which can be represented as a 1×1 tensor) with a matrix? Or a vector of size 3 with a 3×4 matrix? In both cases, we can find a way to make sense of this operation.&lt;/p&gt; Broadcasting gives specific rules to codify when shapes are compatible when trying to do an elementwise operation, and how the tensor of the smaller shape is expanded to match the tensor of the bigger shape. It&#39;s essential to master those rules if you want to be able to write code that executes quickly. In this section, we&#39;ll expand our previous treatment of broadcasting to understand these rules. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Broadcasting with a scalar . Broadcasting with a scalar is the easiest type of broadcasting. When we have a tensor a and a scalar, we just imagine a tensor of the same shape as a filled with that scalar and perform the operation: . a = tensor([10., 6, -4]) a &gt; 0 . tensor([ True, True, False]) . How are we able to do this comparison? 0 is being broadcast to have the same dimensions as a. Note that this is done without creating a tensor full of zeros in memory (that would be very inefficient). . This is very useful if you want to normalize your dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar): . m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]) (m - 5) / 2.73 . tensor([[-1.4652, -1.0989, -0.7326], [-0.3663, 0.0000, 0.3663], [ 0.7326, 1.0989, 1.4652]]) . What if have different means for each row of the matrix? in that case you will need to broadcast a vector to a matrix. . Broadcasting a vector to a matrix . We can broadcast a vector to a matrix as follows: . c = tensor([10.,20,30]) m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]) m.shape,c.shape . (torch.Size([3, 3]), torch.Size([3])) . m + c . tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) . Here the elements of c are expanded to make three rows that match, making the operation possible. Again, PyTorch doesn&#39;t actually create three copies of c in memory. This is done by the expand_as method behind the scenes: . c.expand_as(m) . tensor([[10., 20., 30.], [10., 20., 30.], [10., 20., 30.]]) . If we look at the corresponding tensor, we can ask for its storage property (which shows the actual contents of the memory used for the tensor) to check there is no useless data stored: . t = c.expand_as(m) t.storage() . 10.0 20.0 30.0 [torch.FloatStorage of size 3] . Even though the tensor officially has nine elements, only three scalars are stored in memory. This is possible thanks to the clever trick of giving that dimension a stride of 0 (which means that when PyTorch looks for the next row by adding the stride, it doesn&#39;t move): . t.stride(), t.shape . ((0, 1), torch.Size([3, 3])) . Since m is of size 3×3, there are two ways to do broadcasting. The fact it was done on the last dimension is a convention that comes from the rules of broadcasting and has nothing to do with the way we ordered our tensors. If instead we do this, we get the same result: . c + m . tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) . In fact, it&#39;s only possible to broadcast a vector of size n with a matrix of size m by n: . c = tensor([10.,20,30]) m = tensor([[1., 2, 3], [4,5,6]]) c+m . tensor([[11., 22., 33.], [14., 25., 36.]]) . This won&#39;t work: . c = tensor([10.,20]) m = tensor([[1., 2, 3], [4,5,6]]) c+m . RuntimeError Traceback (most recent call last) &lt;ipython-input-25-64bbbad4d99c&gt; in &lt;module&gt; 1 c = tensor([10.,20]) 2 m = tensor([[1., 2, 3], [4,5,6]]) -&gt; 3 c+m RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1 . If we want to broadcast in the other dimension, we have to change the shape of our vector to make it a 3×1 matrix. This is done with the unsqueeze method in PyTorch: . c = tensor([10.,20,30]) m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]) c = c.unsqueeze(1) m.shape,c.shape . (torch.Size([3, 3]), torch.Size([3, 1])) . This time, c is expanded on the column side: . c+m . tensor([[11., 12., 13.], [24., 25., 26.], [37., 38., 39.]]) . Like before, only three scalars are stored in memory: . t = c.expand_as(m) t.storage() . 10.0 20.0 30.0 [torch.FloatStorage of size 3] . And the expanded tensor has the right shape because the column dimension has a stride of 0: . t.stride(), t.shape . ((1, 0), torch.Size([3, 3])) . With broadcasting, by default if we need to add dimensions, they are added at the beginning. When we were broadcasting before, Pytorch was doing c.unsqueeze(0) behind the scenes: . c = tensor([10.,20,30]) c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape . (torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1])) . The unsqueeze command can be replaced by None indexing: . c.shape, c[None,:].shape,c[:,None].shape . (torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1])) . You can always omit trailing colons, and ... means all preceding dimensions: . c[None].shape,c[...,None].shape . (torch.Size([1, 3]), torch.Size([3, 1])) . With this, we can remove another for loop in our matrix multiplication function. Now, instead of multiplying a[i] with b[:,j], we can multiply a[i] with the whole matrix b using broadcasting, then sum the results: . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): # c[i,j] = (a[i,:] * b[:,j]).sum() # previous c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) return c . %timeit -n 20 t4 = matmul(m1,m2) . 357 µs ± 7.2 µs per loop (mean ± std. dev. of 7 runs, 20 loops each) . We&#39;re now 3,700 times faster than our first implementation! Before we move on, let&#39;s discuss the rules of broadcasting in a little more detail. . Broadcasting rules . When operating on two tensors, PyTorch compares their shapes elementwise. It starts with the trailing dimensions and works its way backward, adding 1 when it meets empty dimensions. Two dimensions are compatible when one of the following is true: . They are equal. | One of them is 1, in which case that dimension is broadcast to make it the same as the other. | . Arrays do not need to have the same number of dimensions. For example, if you have a 256×256×3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with three values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible: . Image (3d tensor): 256 x 256 x 3 Scale (1d tensor): (1) (1) 3 Result (3d tensor): 256 x 256 x 3 . However, a 2D tensor of size 256×256 isn&#39;t compatible with our image: . Image (3d tensor): 256 x 256 x 3 Scale (2d tensor): (1) 256 x 256 Error . In our earlier examples we had with a 3×3 matrix and a vector of size 3, broadcasting was done on the rows: . Matrix (2d tensor): 3 x 3 Vector (1d tensor): (1) 3 Result (2d tensor): 3 x 3 . As an exercise, try to determine what dimensions to add (and where) when you need to normalize a batch of images of size 64 x 3 x 256 x 256 with vectors of three elements (one for the mean and one for the standard deviation). . Another useful way of simplifying tensor manipulations is the use of Einstein summations convention. . Einstein Summation . Before using the PyTorch operation @ or torch.matmul, there is one last way we can implement matrix multiplication: Einstein summation (einsum). This is a compact representation for combining products and sums in a general way. We write an equation like this: . ik,kj -&gt; ij . The lefthand side represents the operands dimensions, separated by commas. Here we have two tensors that each have two dimensions (i,k and k,j). The righthand side represents the result dimensions, so here we have a tensor with two dimensions i,j. . The rules of Einstein summation notation are as follows: . Repeated indices on the left side are implicitly summed over if they are not on the right side. | Each index can appear at most twice on the left side. | The unrepeated indices on the left side must appear on the right side. | So in our example, since k is repeated, we sum over that index. In the end the formula represents the matrix obtained when we put in (i,j) the sum of all the coefficients (i,k) in the first tensor multiplied by the coefficients (k,j) in the second tensor... which is the matrix product! Here is how we can code this in PyTorch: . def matmul(a,b): return torch.einsum(&#39;ik,kj-&gt;ij&#39;, a, b) . Einstein summation is a very practical way of expressing operations involving indexing and sum of products. Note that you can have just one member on the lefthand side. For instance, this: . torch.einsum(&#39;ij-&gt;ji&#39;, a) . returns the transpose of the matrix a. You can also have three or more members. This: . torch.einsum(&#39;bi,ij,bj-&gt;b&#39;, a, b, c) . will return a vector of size b where the k-th coordinate is the sum of a[k,i] b[i,j] c[k,j]. This notation is particularly convenient when you have more dimensions because of batches. For example, if you have two batches of matrices and want to compute the matrix product per batch, you would could this: . torch.einsum(&#39;bik,bkj-&gt;bij&#39;, a, b) . Let&#39;s go back to our new matmul implementation using einsum and look at its speed: . %timeit -n 20 t5 = matmul(m1,m2) . 68.7 µs ± 4.06 µs per loop (mean ± std. dev. of 7 runs, 20 loops each) . As you can see, not only is it practical, but it&#39;s very fast. einsum is often the fastest way to do custom operations in PyTorch, without diving into C++ and CUDA. (But it&#39;s generally not as fast as carefully optimized CUDA code, as you see from the results in &quot;Matrix Multiplication from Scratch&quot;.) . Now that we know how to implement a matrix multiplication from scratch, we are ready to build our neural net—specifically its forward and backward passes—using just matrix multiplications. . The Forward and Backward Passes . As we saw in &lt;&gt;, to train a model, we will need to compute all the gradients of a given loss with respect to its parameters, which is known as the backward pass. The forward pass is where we compute the output of the model on a given input, based on the matrix products. As we define our first neural net, we will also delve into the problem of properly initializing the weights, which is crucial for making training start properly.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Defining and Initializing a Layer . We will take the example of a two-layer neural net first. As we&#39;ve seen, one layer can be expressed as y = x @ w + b, with x our inputs, y our outputs, w the weights of the layer (which is of size number of inputs by number of neurons if we don&#39;t transpose like before), and b is the bias vector: . def lin(x, w, b): return x @ w + b . We can stack the second layer on top of the first, but since mathematically the composition of two linear operations is another linear operation, this only makes sense if we put something nonlinear in the middle, called an activation function. As mentioned at the beginning of the chapter, in deep learning applications the activation function most commonly used is a ReLU, which returns the maximum of x and 0. . We won&#39;t actually train our model in this chapter, so we&#39;ll use random tensors for our inputs and targets. Let&#39;s say our inputs are 200 vectors of size 100, which we group into one batch, and our targets are 200 random floats: . x = torch.randn(200, 100) y = torch.randn(200) . For our two-layer model we will need two weight matrices and two bias vectors. Let&#39;s say we have a hidden size of 50 and the output size is 1 (for one of our inputs, the corresponding output is one float in this toy example). We initialize the weights randomly and the bias at zero: . w1 = torch.randn(100,50) b1 = torch.zeros(50) w2 = torch.randn(50,1) b2 = torch.zeros(1) . Then the result of our first layer is simply: . l1 = lin(x, w1, b1) l1.shape . torch.Size([200, 50]) . Note that this formula works with our batch of inputs, and returns a batch of hidden state: l1 is a matrix of size 200 (our batch size) by 50 (our hidden size). . There is a problem with the way our model was initialized, however. To understand it, we need to look at the mean and standard deviation (std) of l1: . l1.mean(), l1.std() . (tensor(0.0019), tensor(10.1058)) . The mean is close to zero, which is understandable since both our input and weight matrices have means close to zero. But the standard deviation, which represents how far away our activations go from the mean, went from 1 to 10. This is a really big problem because that&#39;s with just one layer. Modern neural nets can have hundred of layers, so if each of them multiplies the scale of our activations by 10, by the end of the last layer we won&#39;t have numbers representable by a computer. . Indeed, if we make just 50 multiplications between x and random matrices of size 100×100, we&#39;ll have: . x = torch.randn(200, 100) for i in range(50): x = x @ torch.randn(100,100) x[0:5,0:5] . tensor([[nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan]]) . The result is nans everywhere. So maybe the scale of our matrix was too big, and we need to have smaller weights? But if we use too small weights, we will have the opposite problem—the scale of our activations will go from 1 to 0.1, and after 50 layers we&#39;ll be left with zeros everywhere: . x = torch.randn(200, 100) for i in range(50): x = x @ (torch.randn(100,100) * 0.01) x[0:5,0:5] . tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) . So we have to scale our weight matrices exactly right so that the standard deviation of our activations stays at 1. We can compute the exact value to use mathematically, as illustrated by Xavier Glorot and Yoshua Bengio in &quot;Understanding the Difficulty of Training Deep Feedforward Neural Networks&quot;. The right scale for a given layer is $1/ sqrt{n_{in}}$, where $n_{in}$ represents the number of inputs. . In our case, if we have 100 inputs, we should scale our weight matrices by 0.1: . x = torch.randn(200, 100) for i in range(50): x = x @ (torch.randn(100,100) * 0.1) x[0:5,0:5] . tensor([[ 0.7554, 0.6167, -0.1757, -1.5662, 0.5644], [-0.1987, 0.6292, 0.3283, -1.1538, 0.5416], [ 0.6106, 0.2556, -0.0618, -0.9463, 0.4445], [ 0.4484, 0.7144, 0.1164, -0.8626, 0.4413], [ 0.3463, 0.5930, 0.3375, -0.9486, 0.5643]]) . Finally some numbers that are neither zeros nor nans! Notice how stable the scale of our activations is, even after those 50 fake layers: . x.std() . tensor(0.7042) . If you play a little bit with the value for scale you&#39;ll notice that even a slight variation from 0.1 will get you either to very small or very large numbers, so initializing the weights properly is extremely important. . Let&#39;s go back to our neural net. Since we messed a bit with our inputs, we need to redefine them: . x = torch.randn(200, 100) y = torch.randn(200) . And for our weights, we&#39;ll use the right scale, which is known as Xavier initialization (or Glorot initialization): . from math import sqrt w1 = torch.randn(100,50) / sqrt(100) b1 = torch.zeros(50) w2 = torch.randn(50,1) / sqrt(50) b2 = torch.zeros(1) . Now if we compute the result of the first layer, we can check that the mean and standard deviation are under control: . l1 = lin(x, w1, b1) l1.mean(),l1.std() . (tensor(-0.0050), tensor(1.0000)) . Very good. Now we need to go through a ReLU, so let&#39;s define one. A ReLU removes the negatives and replaces them with zeros, which is another way of saying it clamps our tensor at zero: . def relu(x): return x.clamp_min(0.) . We pass our activations through this: . l2 = relu(l1) l2.mean(),l2.std() . (tensor(0.3961), tensor(0.5783)) . And we&#39;re back to square one: the mean of our activations has gone to 0.4 (which is understandable since we removed the negatives) and the std went down to 0.58. So like before, after a few layers we will probably wind up with zeros: . x = torch.randn(200, 100) for i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1)) x[0:5,0:5] . tensor([[0.0000e+00, 1.9689e-08, 4.2820e-08, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.6701e-08, 4.3501e-08, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.0976e-08, 3.0411e-08, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.8457e-08, 4.9469e-08, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.9949e-08, 4.1643e-08, 0.0000e+00, 0.0000e+00]]) . This means our initialization wasn&#39;t right. Why? At the time Glorot and Bengio wrote their article, the popular activation in a neural net was the hyperbolic tangent (tanh, which is the one they used), and that initialization doesn&#39;t account for our ReLU. Fortunately, someone else has done the math for us and computed the right scale for us to use. In &quot;Delving Deep into Rectifiers: Surpassing Human-Level Performance&quot; (which we&#39;ve seen before—it&#39;s the article that introduced the ResNet), Kaiming He et al. show that we should use the following scale instead: $ sqrt{2 / n_{in}}$, where $n_{in}$ is the number of inputs of our model. Let&#39;s see what this gives us: . x = torch.randn(200, 100) for i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100))) x[0:5,0:5] . tensor([[0.2871, 0.0000, 0.0000, 0.0000, 0.0026], [0.4546, 0.0000, 0.0000, 0.0000, 0.0015], [0.6178, 0.0000, 0.0000, 0.0180, 0.0079], [0.3333, 0.0000, 0.0000, 0.0545, 0.0000], [0.1940, 0.0000, 0.0000, 0.0000, 0.0096]]) . That&#39;s better: our numbers aren&#39;t all zeroed this time. So let&#39;s go back to the definition of our neural net and use this initialization (which is named Kaiming initialization or He initialization): . x = torch.randn(200, 100) y = torch.randn(200) . w1 = torch.randn(100,50) * sqrt(2 / 100) b1 = torch.zeros(50) w2 = torch.randn(50,1) * sqrt(2 / 50) b2 = torch.zeros(1) . Let&#39;s look at the scale of our activations after going through the first linear layer and ReLU: . l1 = lin(x, w1, b1) l2 = relu(l1) l2.mean(), l2.std() . (tensor(0.5661), tensor(0.8339)) . Much better! Now that our weights are properly initialized, we can define our whole model: . def model(x): l1 = lin(x, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 . This is the forward pass. Now all that&#39;s left to do is to compare our output to the labels we have (random numbers, in this example) with a loss function. In this case, we will use the mean squared error. (It&#39;s a toy problem, and this is the easiest loss function to use for what is next, computing the gradients.) . The only subtlety is that our outputs and targets don&#39;t have exactly the same shape—after going though the model, we get an output like this: . out = model(x) out.shape . torch.Size([200, 1]) . To get rid of this trailing 1 dimension, we use the squeeze function: . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() . And now we are ready to compute our loss: . loss = mse(out, y) . That&#39;s all for the forward pass—let&#39;s now look at the gradients. . Gradients and the Backward Pass . We&#39;ve seen that PyTorch computes all the gradients we need with a magic call to loss.backward, but let&#39;s explore what&#39;s happening behind the scenes. . Now comes the part where we need to compute the gradients of the loss with respect to all the weights of our model, so all the floats in w1, b1, w2, and b2. For this, we will need a bit of math—specifically the chain rule. This is the rule of calculus that guides how we can compute the derivative of a composed function: . $$(g circ f)&#39;(x) = g&#39;(f(x)) f&#39;(x)$$ . j:I find this notation very hard to wrap my head around, so instead I like to think of it as: if y = g(u) and u=f(x); then dy/dx = dy/du * du/dx. The two notations mean the same thing, so use whatever works for you. . Our loss is a big composition of different functions: mean squared error (which is in turn the composition of a mean and a power of two), the second linear layer, a ReLU and the first linear layer. For instance, if we want the gradients of the loss with respect to b2 and our loss is defined by: . loss = mse(out,y) = mse(lin(l2, w2, b2), y) . The chain rule tells us that we have: $$ frac{ text{d} loss}{ text{d} b_{2}} = frac{ text{d} loss}{ text{d} out} times frac{ text{d} out}{ text{d} b_{2}} = frac{ text{d}}{ text{d} out} mse(out, y) times frac{ text{d}}{ text{d} b_{2}} lin(l_{2}, w_{2}, b_{2})$$ . To compute the gradients of the loss with respect to $b_{2}$, we first need the gradients of the loss with respect to our output $out$. It&#39;s the same if we want the gradients of the loss with respect to $w_{2}$. Then, to get the gradients of the loss with respect to $b_{1}$ or $w_{1}$, we will need the gradients of the loss with respect to $l_{1}$, which in turn requires the gradients of the loss with respect to $l_{2}$, which will need the gradients of the loss with respect to $out$. . So to compute all the gradients we need for the update, we need to begin from the output of the model and work our way backward, one layer after the other—which is why this step is known as backpropagation. We can automate it by having each function we implemented (relu, mse, lin) provide its backward step: that is, how to derive the gradients of the loss with respect to the input(s) from the gradients of the loss with respect to the output. . Here we populate those gradients in an attribute of each tensor, a bit like PyTorch does with .grad. . The first are the gradients of the loss with respect to the output of our model (which is the input of the loss function). We undo the squeeze we did in mse, then we use the formula that gives us the derivative of $x^{2}$: $2x$. The derivative of the mean is just $1/n$ where $n$ is the number of elements in our input: . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] . For the gradients of the ReLU and our linear layer, we use the gradients of the loss with respect to the output (in out.g) and apply the chain rule to compute the gradients of the loss with respect to the input (in inp.g). The chain rule tells us that inp.g = relu&#39;(inp) * out.g. The derivative of relu is either 0 (when inputs are negative) or 1 (when inputs are positive), so this gives us: . def relu_grad(inp, out): # grad of relu with respect to input activations inp.g = (inp&gt;0).float() * out.g . The scheme is the same to compute the gradients of the loss with respect to the inputs, weights, and bias in the linear layer: . def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.g = out.g @ w.t() w.g = inp.t() @ out.g b.g = out.g.sum(0) . We won&#39;t linger on the mathematical formulas that define them since they&#39;re not important for our purposes, but do check out Khan Academy&#39;s excellent calculus lessons if you&#39;re interested in this topic. . Sidebar: SymPy . SymPy is a library for symbolic computation that is extremely useful library when working with calculus. Per the documentation: . :Symbolic computation deals with the computation of mathematical objects symbolically. This means that the mathematical objects are represented exactly, not approximately, and mathematical expressions with unevaluated variables are left in symbolic form. . To do symbolic computation, we first define a symbol, and then do a computation, like so: . from sympy import symbols,diff sx,sy = symbols(&#39;sx sy&#39;) diff(sx**2, sx) . $ displaystyle 2 sx$ Here, SymPy has taken the derivative of x**2 for us! It can take the derivative of complicated compound expressions, simplify and factor equations, and much more. There&#39;s really not much reason for anyone to do calculus manually nowadays—for calculating gradients, PyTorch does it for us, and for showing the equations, SymPy does it for us! . End sidebar . Once we have have defined those functions, we can use them to write the backward pass. Since each gradient is automatically populated in the right tensor, we don&#39;t need to store the results of those _grad functions anywhere—we just need to execute them in the reverse order of the forward pass, to make sure that in each function out.g exists: . def forward_and_backward(inp, targ): # forward pass: l1 = inp @ w1 + b1 l2 = relu(l1) out = l2 @ w2 + b2 # we don&#39;t actually need the loss in backward! loss = mse(out, targ) # backward pass: mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . And now we can access the gradients of our model parameters in w1.g, b1.g, w2.g, and b2.g. . We have successfully defined our model—now let&#39;s make it a bit more like a PyTorch module. . Refactoring the Model . The three functions we used have two associated functions: a forward pass and a backward pass. Instead of writing them separately, we can create a class to wrap them together. That class can also store the inputs and outputs for the backward pass. This way, we will just have to call backward: . class Relu(): def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.) return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g . __call__ is a magic name in Python that will make our class callable. This is what will be executed when we type y = Relu()(x). We can do the same for our linear layer and the MSE loss: . class Lin(): def __init__(self, w, b): self.w,self.b = w,b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() self.w.g = self.inp.t() @ self.out.g self.b.g = self.out.g.sum(0) . class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): x = (self.inp.squeeze()-self.targ).unsqueeze(-1) self.inp.g = 2.*x/self.targ.shape[0] . Then we can put everything in a model that we initiate with our tensors w1, b1, w2, b2: . class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . What is really nice about this refactoring and registering things as layers of our model is that the forward and backward passes are now really easy to write. If we want to instantiate our model, we just need to write: . model = Model(w1, b1, w2, b2) . The forward pass can then be executed with: . loss = model(x, y) . And the backward pass with: . model.backward() . Going to PyTorch . The Lin, Mse and Relu classes we wrote have a lot in common, so we could make them all inherit from the same base class: . class LayerFunction(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def bwd(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . Then we just need to implement forward and bwd in each of our subclasses: . class Relu(LayerFunction): def forward(self, inp): return inp.clamp_min(0.) def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g . class Lin(LayerFunction): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = inp.t() @ self.out.g self.b.g = out.g.sum(0) . class Mse(LayerFunction): def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean() def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0] . The rest of our model can be the same as before. This is getting closer and closer to what PyTorch does. Each basic function we need to differentiate is written as a torch.autograd.Function object that has a forward and a backward method. PyTorch will then keep trace of any computation we do to be able to properly run the backward pass, unless we set the requires_grad attribute of our tensors to False. . Writing one of these is (almost) as easy as writing our original classes. The difference is that we choose what to save and what to put in a context variable (so that we make sure we don&#39;t save anything we don&#39;t need), and we return the gradients in the backward pass. It&#39;s very rare to have to write your own Function but if you ever need something exotic or want to mess with the gradients of a regular function, here is how to write one: . from torch.autograd import Function class MyRelu(Function): @staticmethod def forward(ctx, i): result = i.clamp_min(0.) ctx.save_for_backward(i) return result @staticmethod def backward(ctx, grad_output): i, = ctx.saved_tensors return grad_output * (i&gt;0).float() . The structure used to build a more complex model that takes advantage of those Functions is a torch.nn.Module. This is the base structure for all models, and all the neural nets you have seen up until now inherited from that class. It mostly helps to register all the trainable parameters, which as we&#39;ve seen can be used in the training loop. . To implement an nn.Module you just need to: . Make sure the superclass __init__ is called first when you initialize it. | Define any parameters of the model as attributes with nn.Parameter. | Define a forward function that returns the output of your model. | . As an example, here is the linear layer from scratch: . import torch.nn as nn class LinearLayer(nn.Module): def __init__(self, n_in, n_out): super().__init__() self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in)) self.bias = nn.Parameter(torch.zeros(n_out)) def forward(self, x): return x @ self.weight.t() + self.bias . As you see, this class automatically keeps track of what parameters have been defined: . lin = LinearLayer(10,2) p1,p2 = lin.parameters() p1.shape,p2.shape . (torch.Size([2, 10]), torch.Size([2])) . It is thanks to this feature of nn.Module that we can just say opt.step() and have an optimizer loop through the parameters and update each one. . Note that in PyTorch, the weights are stored as an n_out x n_in matrix, which is why we have the transpose in the forward pass. . By using the linear layer from PyTorch (which uses the Kaiming initialization as well), the model we have been building up during this chapter can be written like this: . class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.layers = nn.Sequential( nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)) self.loss = mse def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ) . fastai provides its own variant of Module that is identical to nn.Module, but doesn&#39;t require you to call super().__init__() (it does that for you automatically): . class Model(Module): def __init__(self, n_in, nh, n_out): self.layers = nn.Sequential( nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)) self.loss = mse def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ) . In the last chapter, we will start from such a model and see how to build a training loop from scratch and refactor it to what we&#39;ve been using in previous chapters. . Conclusion . In this chapter we explored the foundations of deep learning, beginning with matrix multiplication and moving on to implementing the forward and backward passes of a neural net from scratch. We then refactored our code to show how PyTorch works beneath the hood. . Here are a few things to remember: . A neural net is basically a bunch of matrix multiplications with nonlinearities in between. | Python is slow, so to write fast code we have to vectorize it and take advantage of techniques such as elementwise arithmetic and broadcasting. | Two tensors are broadcastable if the dimensions starting from the end and going backward match (if they are the same, or one of them is 1). To make tensors broadcastable, we may need to add dimensions of size 1 with unsqueeze or a None index. | Properly initializing a neural net is crucial to get training started. Kaiming initialization should be used when we have ReLU nonlinearities. | The backward pass is the chain rule applied multiple times, computing the gradients from the output of our model and going back, one layer at a time. | When subclassing nn.Module (if not using fastai&#39;s Module) we have to call the superclass __init__ method in our __init__ method and we have to define a forward function that takes an input and returns the desired result. | . Questionnaire . Write the Python code to implement a single neuron. | Write the Python code to implement ReLU. | Write the Python code for a dense layer in terms of matrix multiplication. | Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python). | What is the &quot;hidden size&quot; of a layer? | What does the t method do in PyTorch? | Why is matrix multiplication written in plain Python very slow? | In matmul, why is ac==br? | In Jupyter Notebook, how do you measure the time taken for a single cell to execute? | What is &quot;elementwise arithmetic&quot;? | Write the PyTorch code to test whether every element of a is greater than the corresponding element of b. | What is a rank-0 tensor? How do you convert it to a plain Python data type? | What does this return, and why? tensor([1,2]) + tensor([1]) | What does this return, and why? tensor([1,2]) + tensor([1,2,3]) | How does elementwise arithmetic help us speed up matmul? | What are the broadcasting rules? | What is expand_as? Show an example of how it can be used to match the results of broadcasting. | How does unsqueeze help us to solve certain broadcasting problems? | How can we use indexing to do the same operation as unsqueeze? | How do we show the actual contents of the memory used for a tensor? | When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.) | Do broadcasting and expand_as result in increased memory use? Why or why not? | Implement matmul using Einstein summation. | What does a repeated index letter represent on the left-hand side of einsum? | What are the three rules of Einstein summation notation? Why? | What are the forward pass and backward pass of a neural network? | Why do we need to store some of the activations calculated for intermediate layers in the forward pass? | What is the downside of having activations with a standard deviation too far away from 1? | How can weight initialization help avoid this problem? | What is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU? | Why do we sometimes have to use the squeeze method in loss functions? | What does the argument to the squeeze method do? Why might it be important to include this argument, even though PyTorch does not require it? | What is the &quot;chain rule&quot;? Show the equation in either of the two forms presented in this chapter. | Show how to calculate the gradients of mse(lin(l2, w2, b2), y) using the chain rule. | What is the gradient of ReLU? Show it in math or code. (You shouldn&#39;t need to commit this to memory—try to figure it using your knowledge of the shape of the function.) | In what order do we need to call the *_grad functions in the backward pass? Why? | What is __call__? | What methods must we implement when writing a torch.autograd.Function? | Write nn.Linear from scratch, and test it works. | What is the difference between nn.Module and fastai&#39;s Module? | Further Research . Implement ReLU as a torch.autograd.Function and train a model with it. | If you are mathematically inclined, find out what the gradients of a linear layer are in mathematical notation. Map that to the implementation we saw in this chapter. | Learn about the unfold method in PyTorch, and use it along with matrix multiplication to implement your own 2D convolution function. Then train a CNN that uses it. | Implement everything in this chapter using NumPy instead of PyTorch. | &lt;/div&gt; .",
            "url": "https://pranath.github.io/fastbook/2021/02/19/17_foundations.html",
            "relUrl": "/2021/02/19/17_foundations.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Data Munging with fastai's Mid-Level API",
            "content": "[[chapter_midlevel_data]] We have seen what Tokenizer and Numericalize do to a collection of texts, and how they&#39;re used inside the data block API, which handles those transforms for us directly using the TextBlock. But what if we want to only apply one of those transforms, either to see intermediate results or because we have already tokenized texts? More generally, what can we do when the data block API is not flexible enough to accommodate our particular use case? For this, we need to use fastai&#39;s mid-level API for processing data. The data block API is built on top of that layer, so it will allow you to do everything the data block API does, and much much more. . Going Deeper into fastai&#39;s Layered API . The fastai library is built on a layered API. In the very top layer there are applications that allow us to train a model in five lines of codes, as we saw in &lt;&gt;. In the case of creating DataLoaders for a text classifier, for instance, we used the line:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) . The factory method TextDataLoaders.from_folder is very convenient when your data is arranged the exact same way as the IMDb dataset, but in practice, that often won&#39;t be the case. The data block API offers more flexibility. As we saw in the last chapter, we can get the same result with: . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . But it&#39;s sometimes not flexible enough. For debugging purposes, for instance, we might need to apply just parts of the transforms that come with this data block. Or we might want to create a DataLoaders for some application that isn&#39;t directly supported by fastai. In this section, we&#39;ll dig into the pieces that are used inside fastai to implement the data block API. Understanding these will enable you to leverage the power and flexibility of this mid-tier API. . . Note: Mid-Level API: The mid-level API does not only contain functionality for creating DataLoaders. It also has the callback system, which allows us to customize the training loop any way we like, and the general optimizer. Both will be covered in &lt;&gt;. &lt;/div&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Transforms . When we studied tokenization and numericalization in the last chapter, we started by grabbing a bunch of texts: . files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) txts = L(o.open().read() for o in files[:2000]) . We then showed how to tokenize them with a Tokenizer: . tok = Tokenizer.from_folder(path) tok.setup(txts) toks = txts.map(tok) toks[0] . (#374) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;well&#39;,&#39;,&#39;,&#39;&#34;&#39;,&#39;cube&#39;,&#39;&#34;&#39;,&#39;(&#39;,&#39;1997&#39;,&#39;)&#39;...] . and how to numericalize, including automatically creating the vocab for our corpus: . num = Numericalize() num.setup(toks) nums = toks.map(num) nums[0][:10] . tensor([ 2, 8, 76, 10, 23, 3112, 23, 34, 3113, 33]) . The classes also have a decode method. For instance, Numericalize.decode gives us back the string tokens: . nums_dec = num.decode(nums[0][:10]); nums_dec . (#10) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;well&#39;,&#39;,&#39;,&#39;&#34;&#39;,&#39;cube&#39;,&#39;&#34;&#39;,&#39;(&#39;,&#39;1997&#39;,&#39;)&#39;] . and Tokenizer.decode turns this back into a single string (it may not, however, be exactly the same as the original string; this depends on whether the tokenizer is reversible, which the default word tokenizer is not at the time we&#39;re writing this book): . tok.decode(nums_dec) . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 )&#39; . decode is used by fastai&#39;s show_batch and show_results, as well as some other inference methods, to convert predictions and mini-batches into a human-understandable representation. . For each of tok or num in the preceding example, we created an object, called the setup method (which trains the tokenizer if needed for tok and creates the vocab for num), applied it to our raw texts (by calling the object as a function), and then finally decoded the result back to an understandable representation. These steps are needed for most data preprocessing tasks, so fastai provides a class that encapsulates them. This is the Transform class. Both Tokenize and Numericalize are Transforms. . In general, a Transform is an object that behaves like a function and has an optional setup method that will initialize some inner state (like the vocab inside num) and an optional decode that will reverse the function (this reversal may not be perfect, as we saw with tok). . A good example of decode is found in the Normalize transform that we saw in &lt;&gt;: to be able to plot the images its decode method undoes the normalization (i.e., it multiplies by the standard deviation and adds back the mean). On the other hand, data augmentation transforms do not have a decode method, since we want to show the effects on images to make sure the data augmentation is working as we want.&lt;/p&gt; A special behavior of Transforms is that they always get applied over tuples. In general, our data is always a tuple (input,target) (sometimes with more than one input or more than one target). When applying a transform on an item like this, such as Resize, we don&#39;t want to resize the tuple as a whole; instead, we want to resize the input (if applicable) and the target (if applicable) separately. It&#39;s the same for batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target. . We can see this behavior if we pass a tuple of texts to tok: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; tok((txts[0], txts[1])) . ((#374) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;well&#39;,&#39;,&#39;,&#39;&#34;&#39;,&#39;cube&#39;,&#39;&#34;&#39;,&#39;(&#39;,&#39;1997&#39;,&#39;)&#39;...], (#207) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;conrad&#39;,&#39;xxmaj&#39;,&#39;hall&#39;,&#39;went&#39;,&#39;out&#39;,&#39;with&#39;,&#39;a&#39;,&#39;bang&#39;...]) . Writing Your Own Transform . If you want to write a custom transform to apply to your data, the easiest way is to write a function. As you can see in this example, a Transform will only be applied to a matching type, if a type is provided (otherwise it will always be applied). In the following code, the :int in the function signature means that f only gets applied to ints. That&#39;s why tfm(2.0) returns 2.0, but tfm(2) returns 3 here: . def f(x:int): return x+1 tfm = Transform(f) tfm(2),tfm(2.0) . (3, 2.0) . Here, f is converted to a Transform with no setup and no decode method. . Python has a special syntax for passing a function (like f) to another function (or something that behaves like a function, known as a callable in Python), called a decorator. A decorator is used by prepending a callable with @ and placing it before a function definition (there are lots of good online tutorials about Python decorators, so take a look at one if this is a new concept for you). The following is identical to the previous code: . @Transform def f(x:int): return x+1 f(2),f(2.0) . (3, 2.0) . If you need either setup or decode, you will need to subclass Transform to implement the actual encoding behavior in encodes, then (optionally), the setup behavior in setups and the decoding behavior in decodes: . class NormalizeMean(Transform): def setups(self, items): self.mean = sum(items)/len(items) def encodes(self, x): return x-self.mean def decodes(self, x): return x+self.mean . Here, NormalizeMean will initialize some state during the setup (the mean of all elements passed), then the transformation is to subtract that mean. For decoding purposes, we implement the reverse of that transformation by adding the mean. Here is an example of NormalizeMean in action: . tfm = NormalizeMean() tfm.setup([1,2,3,4,5]) start = 2 y = tfm(start) z = tfm.decode(y) tfm.mean,y,z . (3.0, -1.0, 2.0) . Note that the method called and the method implemented are different, for each of these methods: . asciidoc [options=&quot;header&quot;] |====== | Class | To call | To implement | `nn.Module` (PyTorch) | `()` (i.e., call as function) | `forward` | `Transform` | `()` | `encodes` | `Transform` | `decode()` | `decodes` | `Transform` | `setup()` | `setups` |====== . So, for instance, you would never call setups directly, but instead would call setup. The reason for this is that setup does some work before and after calling setups for you. To learn more about Transforms and how you can use them to implement different behavior depending on the type of the input, be sure to check the tutorials in the fastai docs. . Pipeline . To compose several transforms together, fastai provides the Pipeline class. We define a Pipeline by passing it a list of Transforms; it will then compose the transforms inside it. When you call Pipeline on an object, it will automatically call the transforms inside, in order: . tfms = Pipeline([tok, num]) t = tfms(txts[0]); t[:20] . tensor([ 2, 8, 76, 10, 23, 3112, 23, 34, 3113, 33, 10, 8, 4477, 22, 88, 32, 10, 27, 42, 14]) . And you can call decode on the result of your encoding, to get back something you can display and analyze: . tfms.decode(t)[:100] . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesti&#39; . The only part that doesn&#39;t work the same way as in Transform is the setup. To properly set up a Pipeline of Transforms on some data, you need to use a TfmdLists. . TfmdLists and Datasets: Transformed Collections . Your data is usually a set of raw items (like filenames, or rows in a DataFrame) to which you want to apply a succession of transformations. We just saw that a succession of transformations is represented by a Pipeline in fastai. The class that groups together this Pipeline with your raw items is called TfmdLists. . TfmdLists . Here is the short way of doing the transformation we saw in the previous section: . tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize]) . At initialization, the TfmdLists will automatically call the setup method of each Transform in order, providing them not with the raw items but the items transformed by all the previous Transforms in order. We can get the result of our Pipeline on any raw element just by indexing into the TfmdLists: . t = tls[0]; t[:20] . tensor([ 2, 8, 91, 11, 22, 5793, 22, 37, 4910, 34, 11, 8, 13042, 23, 107, 30, 11, 25, 44, 14]) . And the TfmdLists knows how to decode for show purposes: . tls.decode(t)[:100] . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesti&#39; . In fact, it even has a show method: . tls.show(t) . xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesting and tricky ideas that xxmaj i &#39;ve ever seen when talking about movies . xxmaj they had just one scenery , a bunch of actors and a plot . xxmaj so , what made it so special were all the effective direction , great dialogs and a bizarre condition that characters had to deal like rats in a labyrinth . xxmaj his second movie , &#34; cypher &#34; ( 2002 ) , was all about its story , but it was n&#39;t so good as &#34; cube &#34; but here are the characters being tested like rats again . &#34; nothing &#34; is something very interesting and gets xxmaj vincenzo coming back to his &#39; cube days &#39; , locking the characters once again in a very different space with no time once more playing with the characters like playing with rats in an experience room . xxmaj but instead of a thriller sci - fi ( even some of the promotional teasers and trailers erroneous seemed like that ) , &#34; nothing &#34; is a loose and light comedy that for sure can be called a modern satire about our society and also about the intolerant world we &#39;re living . xxmaj once again xxmaj xxunk amaze us with a great idea into a so small kind of thing . 2 actors and a blinding white scenario , that &#39;s all you got most part of time and you do n&#39;t need more than that . xxmaj while &#34; cube &#34; is a claustrophobic experience and &#34; cypher &#34; confusing , &#34; nothing &#34; is completely the opposite but at the same time also desperate . xxmaj this movie proves once again that a smart idea means much more than just a millionaire budget . xxmaj of course that the movie fails sometimes , but its prime idea means a lot and offsets any flaws . xxmaj there &#39;s nothing more to be said about this movie because everything is a brilliant surprise and a totally different experience that i had in movies since &#34; cube &#34; . . The TfmdLists is named with an &quot;s&quot; because it can handle a training and a validation set with a splits argument. You just need to pass the indices of which elements are in the training set, and which are in the validation set: . cut = int(len(files)*0.8) splits = [list(range(cut)), list(range(cut,len(files)))] tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], splits=splits) . You can then access them through the train and valid attributes: . tls.valid[0][:20] . tensor([ 2, 8, 20, 30, 87, 510, 1570, 12, 408, 379, 4196, 10, 8, 20, 30, 16, 13, 12216, 202, 509]) . If you have manually written a Transform that performs all of your preprocessing at once, turning raw items into a tuple with inputs and targets, then TfmdLists is the class you need. You can directly convert it to a DataLoaders object with the dataloaders method. This is what we will do in our Siamese example later in this chapter. . In general, though, you will have two (or more) parallel pipelines of transforms: one for processing your raw items into inputs and one to process your raw items into targets. For instance, here, the pipeline we defined only processes the raw text into inputs. If we want to do text classification, we also have to process the labels into targets. . For this we need to do two things. First we take the label name from the parent folder. There is a function, parent_label, for this: . lbls = files.map(parent_label) lbls . (#50000) [&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;...] . Then we need a Transform that will grab the unique items and build a vocab with them during setup, then transform the string labels into integers when called. fastai provides this for us; it&#39;s called Categorize: . cat = Categorize() cat.setup(lbls) cat.vocab, cat(lbls[0]) . ((#2) [&#39;neg&#39;,&#39;pos&#39;], TensorCategory(1)) . To do the whole setup automatically on our list of files, we can create a TfmdLists as before: . tls_y = TfmdLists(files, [parent_label, Categorize()]) tls_y[0] . TensorCategory(1) . But then we end up with two separate objects for our inputs and targets, which is not what we want. This is where Datasets comes to the rescue. . Datasets . Datasets will apply two (or more) pipelines in parallel to the same raw object and build a tuple with the result. Like TfmdLists, it will automatically do the setup for us, and when we index into a Datasets, it will return us a tuple with the results of each pipeline: . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms]) x,y = dsets[0] x[:20],y . Like a TfmdLists, we can pass along splits to a Datasets to split our data between training and validation sets: . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms], splits=splits) x,y = dsets.valid[0] x[:20],y . (tensor([ 2, 8, 20, 30, 87, 510, 1570, 12, 408, 379, 4196, 10, 8, 20, 30, 16, 13, 12216, 202, 509]), TensorCategory(0)) . It can also decode any processed tuple or show it directly: . t = dsets.valid[0] dsets.decode(t) . (&#39;xxbos xxmaj this movie had horrible lighting and terrible camera movements . xxmaj this movie is a jumpy horror flick with no meaning at all . xxmaj the slashes are totally fake looking . xxmaj it looks like some 17 year - old idiot wrote this movie and a 10 year old kid shot it . xxmaj with the worst acting you can ever find . xxmaj people are tired of knives . xxmaj at least move on to guns or fire . xxmaj it has almost exact lines from &#34; when a xxmaj stranger xxmaj calls &#34; . xxmaj with gruesome killings , only crazy people would enjoy this movie . xxmaj it is obvious the writer does n &#39;t have kids or even care for them . i mean at show some mercy . xxmaj just to sum it up , this movie is a &#34; b &#34; movie and it sucked . xxmaj just for your own sake , do n &#39;t even think about wasting your time watching this crappy movie .&#39;, &#39;neg&#39;) . The last step is to convert our Datasets object to a DataLoaders, which can be done with the dataloaders method. Here we need to pass along a special argument to take care of the padding problem (as we saw in the last chapter). This needs to happen just before we batch the elements, so we pass it to before_batch: . dls = dsets.dataloaders(bs=64, before_batch=pad_input) . dataloaders directly calls DataLoader on each subset of our Datasets. fastai&#39;s DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. It has a lot of points of customization, but the most important ones that you should know are: . after_item:: Applied on each item after grabbing it inside the dataset. This is the equivalent of item_tfms in DataBlock. | before_batch:: Applied on the list of items before they are collated. This is the ideal place to pad items to the same size. | after_batch:: Applied on the batch as a whole after its construction. This is the equivalent of batch_tfms in DataBlock. | . As a conclusion, here is the full code necessary to prepare the data for text classification: . tfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]] files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) splits = GrandparentSplitter(valid_name=&#39;test&#39;)(files) dsets = Datasets(files, tfms, splits=splits) dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input) . The two differences from the previous code are the use of GrandparentSplitter to split our training and validation data, and the dl_type argument. This is to tell dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches. . This does the exact same thing as our previous DataBlock: . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . But now, you know how to customize every single piece of it! . Let&#39;s practice what we just learned about this mid-level API for data preprocessing, using a computer vision example now. . Applying the Mid-Level Data API: SiamesePair . A Siamese model takes two images and has to determine if they are of the same class or not. For this example, we will use the Pet dataset again and prepare the data for a model that will have to predict if two images of pets are of the same breed or not. We will explain here how to prepare the data for such a model, then we will train that model in &lt;&gt;.&lt;/p&gt; First things first, let&#39;s get the images in our dataset: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.vision.all import * path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) . If we didn&#39;t care about showing our objects at all, we could directly create one transform to completely preprocess that list of files. We will want to look at those images though, so we need to create a custom type. When you call the show method on a TfmdLists or a Datasets object, it will decode items until it reaches a type that contains a show method and use it to show the object. That show method gets passed a ctx, which could be a matplotlib axis for images, or a row of a DataFrame for texts. . Here we create a SiameseImage object that subclasses fastuple and is intended to contain three things: two images, and a Boolean that&#39;s True if the images are of the same breed. We also implement the special show method, such that it concatenates the two images with a black line in the middle. Don&#39;t worry too much about the part that is in the if test (which is to show the SiameseImage when the images are Python images, not tensors); the important part is in the last three lines: . class SiameseImage(fastuple): def show(self, ctx=None, **kwargs): img1,img2,same_breed = self if not isinstance(img1, Tensor): if img2.size != img1.size: img2 = img2.resize(img1.size) t1,t2 = tensor(img1),tensor(img2) t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1) else: t1,t2 = img1,img2 line = t1.new_zeros(t1.shape[0], t1.shape[1], 10) return show_image(torch.cat([t1,line,t2], dim=2), title=same_breed, ctx=ctx) . Let&#39;s create a first SiameseImage and check our show method works: . img = PILImage.create(files[0]) s = SiameseImage(img, img, True) s.show(); . We can also try with a second image that&#39;s not from the same class: . img1 = PILImage.create(files[1]) s1 = SiameseImage(img, img1, False) s1.show(); . The important thing with transforms that we saw before is that they dispatch over tuples or their subclasses. That&#39;s precisely why we chose to subclass fastuple in this instance—this way we can apply any transform that works on images to our SiameseImage and it will be applied on each image in the tuple: . s2 = Resize(224)(s1) s2.show(); . Here the Resize transform is applied to each of the two images, but not the Boolean flag. Even if we have a custom type, we can thus benefit from all the data augmentation transforms inside the library. . We are now ready to build the Transform that we will use to get our data ready for a Siamese model. First, we will need a function to determine the classes of all our images: . def label_func(fname): return re.match(r&#39;^(.*)_ d+.jpg$&#39;, fname.name).groups()[0] . For each image our tranform will, with a probability of 0.5, draw an image from the same class and return a SiameseImage with a true label, or draw an image from another class and return a SiameseImage with a false label. This is all done in the private _draw function. There is one difference between the training and validation sets, which is why the transform needs to be initialized with the splits: on the training set we will make that random pick each time we read an image, whereas on the validation set we make this random pick once and for all at initialization. This way, we get more varied samples during training, but always the same validation set: . class SiameseTransform(Transform): def __init__(self, files, label_func, splits): self.labels = files.map(label_func).unique() self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels} self.label_func = label_func self.valid = {f: self._draw(f) for f in files[splits[1]]} def encodes(self, f): f2,t = self.valid.get(f, self._draw(f)) img1,img2 = PILImage.create(f),PILImage.create(f2) return SiameseImage(img1, img2, t) def _draw(self, f): same = random.random() &lt; 0.5 cls = self.label_func(f) if not same: cls = random.choice(L(l for l in self.labels if l != cls)) return random.choice(self.lbl2files[cls]),same . We can then create our main transform: . splits = RandomSplitter()(files) tfm = SiameseTransform(files, label_func, splits) tfm(files[0]).show(); . In the mid-level API for data collection we have two objects that can help us apply transforms on a set of items, TfmdLists and Datasets. If you remember what we have just seen, one applies a Pipeline of transforms and the other applies several Pipelines of transforms in parallel, to build tuples. Here, our main transform already builds the tuples, so we use TfmdLists: . tls = TfmdLists(files, tfm, splits=splits) show_at(tls.valid, 0); . And we can finally get our data in DataLoaders by calling the dataloaders method. One thing to be careful of here is that this method does not take item_tfms and batch_tfms like a DataBlock. The fastai DataLoader has several hooks that are named after events; here what we apply on the items after they are grabbed is called after_item, and what we apply on the batch once it&#39;s built is called after_batch: . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . Note that we need to pass more transforms than usual—that&#39;s because the data block API usually adds them automatically: . ToTensor is the one that converts images to tensors (again, it&#39;s applied on every part of the tuple). | IntToFloatTensor converts the tensor of images containing integers from 0 to 255 to a tensor of floats, and divides by 255 to make the values between 0 and 1. | . We can now train a model using this DataLoaders. It will need a bit more customization than the usual model provided by cnn_learner since it has to take two images instead of one, but we will see how to create such a model and train it in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Conclusion . fastai provides a layered API. It takes one line of code to grab the data when it&#39;s in one of the usual settings, making it easy for beginners to focus on training a model without spending too much time assembling the data. Then, the high-level data block API gives you more flexibility by allowing you to mix and match some building blocks. Underneath it, the mid-level API gives you greater flexibility to apply any transformations on your items. In your real-world problems, this is probably what you will need to use, and we hope it makes the step of data-munging as easy as possible. . Questionnaire . Why do we say that fastai has a &quot;layered&quot; API? What does it mean? | Why does a Transform have a decode method? What does it do? | Why does a Transform have a setup method? What does it do? | How does a Transform work when called on a tuple? | Which methods do you need to implement when writing your own Transform? | Write a Normalize transform that fully normalizes items (subtract the mean and divide by the standard deviation of the dataset), and that can decode that behavior. Try not to peek! | Write a Transform that does the numericalization of tokenized texts (it should set its vocab automatically from the dataset seen and have a decode method). Look at the source code of fastai if you need help. | What is a Pipeline? | What is a TfmdLists? | What is a Datasets? How is it different from a TfmdLists? | Why are TfmdLists and Datasets named with an &quot;s&quot;? | How can you build a DataLoaders from a TfmdLists or a Datasets? | How do you pass item_tfms and batch_tfms when building a DataLoaders from a TfmdLists or a Datasets? | What do you need to do when you want to have your custom items work with methods like show_batch or show_results? | Why can we easily apply fastai data augmentation transforms to the SiamesePair we built? | Further Research . Use the mid-level API to prepare the data in DataLoaders on your own datasets. Try this with the Pet dataset and the Adult dataset from Chapter 1. | Look at the Siamese tutorial in the fastai documentation to learn how to customize the behavior of show_batch and show_results for new type of items. Implement it in your own project. | Understanding fastai&#39;s Applications: Wrap Up . Congratulations—you&#39;ve completed all of the chapters in this book that cover the key practical parts of training models and using deep learning! You know how to use all of fastai&#39;s built-in applications, and how to customize them using the data block API and loss functions. You even know how to create a neural network from scratch, and train it! (And hopefully you now know some of the questions to ask to make sure your creations help improve society too.) . The knowledge you already have is enough to create full working prototypes of many types of neural network applications. More importantly, it will help you understand the capabilities and limitations of deep learning models, and how to design a system that&#39;s well adapted to them. . In the rest of this book we will be pulling apart those applications, piece by piece, to understand the foundations they are built on. This is important knowledge for a deep learning practitioner, because it is what allows you to inspect and debug models that you build and create new applications that are customized for your particular projects. . &lt;/div&gt; . .",
            "url": "https://pranath.github.io/fastbook/2021/02/19/11_midlevel_data.html",
            "relUrl": "/2021/02/19/11_midlevel_data.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Other Computer Vision Problems",
            "content": "[[chapter_multicat]] In the previous chapter you learned some important practical techniques for training models in practice. Considerations like selecting learning rates and the number of epochs are very important to getting good results. . In this chapter we are going to look at two other types of computer vision problems: multi-label classification and regression. The first one is when you want to predict more than one label per image (or sometimes none at all), and the second is when your labels are one or several numbers—a quantity instead of a category. . In the process will study more deeply the output activations, targets, and loss functions in deep learning models. . Multi-Label Classification . Multi-label classification refers to the problem of identifying the categories of objects in images that may not contain exactly one type of object. There may be more than one kind of object, or there may be no objects at all in the classes that you are looking for. . For instance, this would have been a great approach for our bear classifier. One problem with the bear classifier that we rolled out in &lt;&gt; was that if a user uploaded something that wasn&#39;t any kind of bear, the model would still say it was either a grizzly, black, or teddy bear—it had no ability to predict &quot;not a bear at all.&quot; In fact, after we have completed this chapter, it would be a great exercise for you to go back to your image classifier application, and try to retrain it using the multi-label technique, then test it by passing in an image that is not of any of your recognized classes.&lt;/p&gt; In practice, we have not seen many examples of people training multi-label classifiers for this purpose—but we very often see both users and developers complaining about this problem. It appears that this simple solution is not at all widely understood or appreciated! Because in practice it is probably more common to have some images with zero matches or more than one match, we should probably expect in practice that multi-label classifiers are more widely applicable than single-label classifiers. . First, let&#39;s see what a multi-label dataset looks like, then we&#39;ll explain how to get it ready for our model. You&#39;ll see that the architecture of the model does not change from the last chapter; only the loss function does. Let&#39;s start with the data. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The Data . For our example we are going to use the PASCAL dataset, which can have more than one kind of classified object per image. . We begin by downloading and extracting the dataset as per usual: . from fastai.vision.all import * path = untar_data(URLs.PASCAL_2007) . This dataset is different from the ones we have seen before, in that it is not structured by filename or folder but instead comes with a CSV (comma-separated values) file telling us what labels to use for each image. We can inspect the CSV file by reading it into a Pandas DataFrame: . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . As you can see, the list of categories in each image is shown as a space-delimited string. . Sidebar: Pandas and DataFrames . No, it’s not actually a panda! Pandas is a Python library that is used to manipulate and analyze tabular and time series data. The main class is DataFrame, which represents a table of rows and columns. You can get a DataFrame from a CSV file, a database table, Python dictionaries, and many other sources. In Jupyter, a DataFrame is output as a formatted table, as shown here. . You can access rows and columns of a DataFrame with the iloc property, as if it were a matrix: . df.iloc[:,0] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . df.iloc[0,:] # Trailing :s are always optional (in numpy, pytorch, pandas, etc.), # so this is equivalent: df.iloc[0] . fname 000005.jpg labels chair is_valid True Name: 0, dtype: object . You can also grab a column by name by indexing into a DataFrame directly: . df[&#39;fname&#39;] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . You can create new columns and do calculations using columns: . tmp_df = pd.DataFrame({&#39;a&#39;:[1,2], &#39;b&#39;:[3,4]}) tmp_df . a b . 0 1 | 3 | . 1 2 | 4 | . tmp_df[&#39;c&#39;] = tmp_df[&#39;a&#39;]+tmp_df[&#39;b&#39;] tmp_df . a b c . 0 1 | 3 | 4 | . 1 2 | 4 | 6 | . Pandas is a fast and flexible library, and an important part of every data scientist’s Python toolbox. Unfortunately, its API can be rather confusing and surprising, so it takes a while to get familiar with it. If you haven’t used Pandas before, we’d suggest going through a tutorial; we are particularly fond of the book Python for Data Analysis by Wes McKinney, the creator of Pandas (O&#39;Reilly). It also covers other important libraries like matplotlib and numpy. We will try to briefly describe Pandas functionality we use as we come across it, but will not go into the level of detail of McKinney’s book. . End sidebar . Now that we have seen what the data looks like, let&#39;s make it ready for model training. . Constructing a DataBlock . How do we convert from a DataFrame object to a DataLoaders object? We generally suggest using the data block API for creating a DataLoaders object, where possible, since it provides a good mix of flexibility and simplicity. Here we will show you the steps that we take to use the data blocks API to construct a DataLoaders object in practice, using this dataset as an example. . As we have seen, PyTorch and fastai have two main classes for representing and accessing a training set or validation set: . Dataset:: A collection that returns a tuple of your independent and dependent variable for a single item | DataLoader:: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables | . On top of these, fastai provides two classes for bringing your training and validation sets together: . Datasets:: An object that contains a training Dataset and a validation Dataset | DataLoaders:: An object that contains a training DataLoader and a validation DataLoader | . Since a DataLoader builds on top of a Dataset and adds additional functionality to it (collating multiple items into a mini-batch), it’s often easiest to start by creating and testing Datasets, and then look at DataLoaders after that’s working. . When we create a DataBlock, we build up gradually, step by step, and use the notebook to check our data along the way. This is a great way to make sure that you maintain momentum as you are coding, and that you keep an eye out for any problems. It’s easy to debug, because you know that if a problem arises, it is in the line of code you just typed! . Let’s start with the simplest case, which is a data block created with no parameters: . dblock = DataBlock() . We can create a Datasets object from this. The only thing needed is a source—in this case, our DataFrame: . dsets = dblock.datasets(df) . This contains a train and a valid dataset, which we can index into: . len(dsets.train),len(dsets.valid) . (4009, 1002) . x,y = dsets.train[0] x,y . (fname 008663.jpg labels car person is_valid False Name: 4346, dtype: object, fname 008663.jpg labels car person is_valid False Name: 4346, dtype: object) . As you can see, this simply returns a row of the DataFrame, twice. This is because by default, the data block assumes we have two things: input and target. We are going to need to grab the appropriate fields from the DataFrame, which we can do by passing get_x and get_y functions: . x[&#39;fname&#39;] . &#39;008663.jpg&#39; . dblock = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets = dblock.datasets(df) dsets.train[0] . (&#39;005620.jpg&#39;, &#39;aeroplane&#39;) . As you can see, rather than defining a function in the usual way, we are using Python’s lambda keyword. This is just a shortcut for defining and then referring to a function. The following more verbose approach is identical: . def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;] dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (&#39;002549.jpg&#39;, &#39;tvmonitor&#39;) . Lambda functions are great for quickly iterating, but they are not compatible with serialization, so we advise you to use the more verbose approach if you want to export your Learner after training (lambdas are fine if you are just experimenting). . We can see that the independent variable will need to be converted into a complete path, so that we can open it as an image, and the dependent variable will need to be split on the space character (which is the default for Python’s split function) so that it becomes a list: . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (Path(&#39;/home/jhoward/.fastai/data/pascal_2007/train/002844.jpg&#39;), [&#39;train&#39;]) . To actually open the image and do the conversion to tensors, we will need to use a set of transforms; block types will provide us with those. We can use the same block types that we have used previously, with one exception: the ImageBlock will work fine again, because we have a path that points to a valid image, but the CategoryBlock is not going to work. The problem is that block returns a single integer, but we need to be able to have multiple labels for each item. To solve this, we use a MultiCategoryBlock. This type of block expects to receive a list of strings, as we have in this case, so let’s test it out: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x375, TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])) . As you can see, our list of categories is not encoded in the same way that it was for the regular CategoryBlock. In that case, we had a single integer representing which category was present, based on its location in our vocab. In this case, however, we instead have a list of zeros, with a one in any position where that category is present. For example, if there is a one in the second and fourth positions, then that means that vocab items two and four are present in this image. This is known as one-hot encoding. The reason we can’t easily just use a list of category indices is that each list would be a different length, and PyTorch requires tensors, where everything has to be the same length. . jargon:One-hot encoding: Using a vector of zeros, with a one in each location that is represented in the data, to encode a list of integers. . Let’s check what the categories represent for this example (we are using the convenient torch.where function, which tells us all of the indices where our condition is true or false): . idxs = torch.where(dsets.train[0][1]==1.)[0] dsets.train.vocab[idxs] . (#1) [&#39;dog&#39;] . With NumPy arrays, PyTorch tensors, and fastai’s L class, we can index directly using a list or vector, which makes a lot of code (such as this example) much clearer and more concise. . We have ignored the column is_valid up until now, which means that DataBlock has been using a random split by default. To explicitly choose the elements of our validation set, we need to write a function and pass it to splitter (or use one of fastai&#39;s predefined functions or classes). It will take the items (here our whole DataFrame) and must return two (or more) lists of integers: . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() valid = df.index[df[&#39;is_valid&#39;]].tolist() return train,valid dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . As we have discussed, a DataLoader collates the items from a Dataset into a mini-batch. This is a tuple of tensors, where each tensor simply stacks the items from that location in the Dataset item. . Now that we have confirmed that the individual items look okay, there&#39;s one more step we need to ensure we can create our DataLoaders, which is to ensure that every item is of the same size. To do this, we can use RandomResizedCrop: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(df) . And now we can display a sample of our data: . dls.show_batch(nrows=1, ncols=3) . Remember that if anything goes wrong when you create your DataLoaders from your DataBlock, or if you want to view exactly what happens with your DataBlock, you can use the summary method we presented in the last chapter. . Our data is now ready for training a model. As we will see, nothing is going to change when we create our Learner, but behind the scenes, the fastai library will pick a new loss function for us: binary cross-entropy. . Binary Cross-Entropy . Now we&#39;ll create our Learner. We saw in &lt;&gt; that a Learner object contains four main things: the model, a DataLoaders object, an Optimizer, and the loss function to use. We already have our DataLoaders, we can leverage fastai&#39;s resnet models (which we&#39;ll learn how to create from scratch later), and we know how to create an SGD optimizer. So let&#39;s focus on ensuring we have a suitable loss function. To do this, let&#39;s use cnn_learner to create a Learner, so we can look at its activations:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = cnn_learner(dls, resnet18) . We also saw that the model in a Learner is generally an object of a class inheriting from nn.Module, and that we can call it using parentheses and it will return the activations of a model. You should pass it your independent variable, as a mini-batch. We can try it out by grabbing a mini batch from our DataLoader and then passing it to the model: . x,y = to_cpu(dls.train.one_batch()) activs = learn.model(x) activs.shape . torch.Size([64, 20]) . Think about why activs has this shape—we have a batch size of 64, and we need to calculate the probability of each of 20 categories. Here’s what one of those activations looks like: . activs[0] . TensorImage([ 0.7476, -1.1988, 4.5421, -1.5915, -0.6749, 0.0343, -2.4930, -0.8330, -0.3817, -1.4876, -0.1683, 2.1547, -3.4151, -1.1743, 0.1530, -1.6801, -2.3067, 0.7063, -1.3358, -0.3715], grad_fn=&lt;AliasBackward&gt;) . . Note: Getting Model Activations: Knowing how to manually get a mini-batch and pass it into a model, and look at the activations and loss, is really important for debugging your model. It is also very helpful for learning, so that you can see exactly what is going on. . They aren’t yet scaled to between 0 and 1, but we learned how to do that in &lt;&gt;, using the sigmoid function. We also saw how to calculate a loss based on this—this is our loss function from &lt;&gt;, with the addition of log as discussed in the last chapter:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, 1-inputs, inputs).log().mean() . Note that because we have a one-hot-encoded dependent variable, we can&#39;t directly use nll_loss or softmax (and therefore we can&#39;t use cross_entropy): . softmax, as we saw, requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (due to the use of exp); however, we may well have multiple objects that we&#39;re confident appear in an image, so restricting the maximum sum of activations to 1 is not a good idea. By the same reasoning, we may want the sum to be less than 1, if we don&#39;t think any of the categories appear in an image. | nll_loss, as we saw, returns the value of just one activation: the single activation corresponding with the single label for an item. This doesn&#39;t make sense when we have multiple labels. | . On the other hand, the binary_cross_entropy function, which is just mnist_loss along with log, provides just what we need, thanks to the magic of PyTorch&#39;s elementwise operations. Each activation will be compared to each target for each column, so we don&#39;t have to do anything to make this function work for multiple columns. . j:One of the things I really like about working with libraries like PyTorch, with broadcasting and elementwise operations, is that quite frequently I find I can write code that works equally well for a single item or a batch of items, without changes. binary_cross_entropy is a great example of this. By using these operations, we don&#39;t have to write loops ourselves, and can rely on PyTorch to do the looping we need as appropriate for the rank of the tensors we&#39;re working with. . PyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names! . F.binary_cross_entropy and its module equivalent nn.BCELoss calculate cross-entropy on a one-hot-encoded target, but do not include the initial sigmoid. Normally for one-hot-encoded targets you&#39;ll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross-entropy in a single function, as in the preceding example. . The equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, is F.nll_loss or nn.NLLLoss for the version without the initial softmax, and F.cross_entropy or nn.CrossEntropyLoss for the version with the initial softmax. . Since we have a one-hot-encoded target, we will use BCEWithLogitsLoss: . loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs, y) loss . TensorImage(1.0342, grad_fn=&lt;AliasBackward&gt;) . We don&#39;t actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders has multiple category labels, so it will use nn.BCEWithLogitsLoss by default. . One change compared to the last chapter is the metric we use: because this is a multilabel problem, we can&#39;t use the accuracy function. Why is that? Well, accuracy was comparing our outputs to our targets like so: . def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred = inp.argmax(dim=axis) return (pred == targ).float().mean() . The class predicted was the one with the highest activation (this is what argmax does). Here it doesn&#39;t work because we could have more than one prediction on a single image. After applying the sigmoid to our activations (to make them between 0 and 1), we need to decide which ones are 0s and which ones are 1s by picking a threshold. Each value above the threshold will be considered as a 1, and each value lower than the threshold will be considered a 0: . def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() . If we pass accuracy_multi directly as a metric, it will use the default value for threshold, which is 0.5. We might want to adjust that default and create a new version of accuracy_multi that has a different default. To help with this, there is a function in Python called partial. It allows us to bind a function with some arguments or keyword arguments, making a new version of that function that, whenever it is called, always includes those arguments. For instance, here is a simple function taking two arguments: . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) . (&#39;Hello Jeremy.&#39;, &#39;Ahoy! Jeremy.&#39;) . We can switch to a French version of that function by using partial: . f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) . (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . We can now train our model. Let&#39;s try setting the accuracy threshold to 0.2 for our metric: . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.942663 | 0.703737 | 0.233307 | 00:08 | . 1 | 0.821548 | 0.550827 | 0.295319 | 00:08 | . 2 | 0.604189 | 0.202585 | 0.816474 | 00:08 | . 3 | 0.359258 | 0.123299 | 0.944283 | 00:08 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.135746 | 0.123404 | 0.944442 | 00:09 | . 1 | 0.118443 | 0.107534 | 0.951255 | 00:09 | . 2 | 0.098525 | 0.104778 | 0.951554 | 00:10 | . Picking a threshold is important. If you pick a threshold that&#39;s too low, you&#39;ll often be failing to select correctly labeled objects. We can see this by changing our metric, and then calling validate, which returns the validation loss and metrics: . learn.metrics = partial(accuracy_multi, thresh=0.1) learn.validate() . (#2) [0.10477833449840546,0.9314740300178528] . If you pick a threshold that&#39;s too high, you&#39;ll only be selecting the objects for which your model is very confident: . learn.metrics = partial(accuracy_multi, thresh=0.99) learn.validate() . (#2) [0.10477833449840546,0.9429482221603394] . We can find the best threshold by trying a few levels and seeing what works best. This is much faster if we just grab the predictions once: . preds,targs = learn.get_preds() . Then we can call the metric directly. Note that by default get_preds applies the output activation function (sigmoid, in this case) for us, so we&#39;ll need to tell accuracy_multi to not apply it: . accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) . TensorImage(0.9567) . We can now use this approach to find the best threshold level: . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . In this case, we&#39;re using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be overfitting to the validation set, since we&#39;re trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we&#39;re clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don&#39;t try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it&#39;s fine to do this). . This concludes the part of this chapter dedicated to multi-label classification. Next, we&#39;ll take a look at a regression problem. . Regression . It&#39;s easy to think of deep learning models as being classified into domains, like computer vision, NLP, and so forth. And indeed, that&#39;s how fastai classifies its applications—largely because that&#39;s how most people are used to thinking of things. . But really, that&#39;s hiding a more interesting and deeper perspective. A model is defined by its independent and dependent variables, along with its loss function. That means that there&#39;s really a far wider array of models than just the simple domain-based split. Perhaps we have an independent variable that&#39;s an image, and a dependent that&#39;s text (e.g., generating a caption from an image); or perhaps we have an independent variable that&#39;s text and dependent that&#39;s an image (e.g., generating an image from a caption—which is actually possible for deep learning to do!); or perhaps we&#39;ve got images, texts, and tabular data as independent variables, and we&#39;re trying to predict product purchases... the possibilities really are endless. . To be able to move beyond fixed applications, to crafting your own novel solutions to novel problems, it helps to really understand the data block API (and maybe also the mid-tier API, which we&#39;ll see later in the book). As an example, let&#39;s consider the problem of image regression. This refers to learning from a dataset where the independent variable is an image, and the dependent variable is one or more floats. Often we see people treat image regression as a whole separate application—but as you&#39;ll see here, we can treat it as just another CNN on top of the data block API. . We&#39;re going to jump straight to a somewhat tricky variant of image regression, because we know you&#39;re ready for it! We&#39;re going to do a key point model. A key point refers to a specific location represented in an image—in this case, we&#39;ll use images of people and we&#39;ll be looking for the center of the person&#39;s face in each image. That means we&#39;ll actually be predicting two values for each image: the row and column of the face center. . Assemble the Data . We will use the Biwi Kinect Head Pose dataset for this section. We&#39;ll begin by downloading the dataset as usual: . path = untar_data(URLs.BIWI_HEAD_POSE) . Let&#39;s see what we&#39;ve got! . path.ls().sorted() . (#50) [Path(&#39;01&#39;),Path(&#39;01.obj&#39;),Path(&#39;02&#39;),Path(&#39;02.obj&#39;),Path(&#39;03&#39;),Path(&#39;03.obj&#39;),Path(&#39;04&#39;),Path(&#39;04.obj&#39;),Path(&#39;05&#39;),Path(&#39;05.obj&#39;)...] . There are 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding .obj file for each (we won&#39;t need them here). Let&#39;s take a look inside one of these directories: . (path/&#39;01&#39;).ls().sorted() . (#1000) [Path(&#39;01/depth.cal&#39;),Path(&#39;01/frame_00003_pose.txt&#39;),Path(&#39;01/frame_00003_rgb.jpg&#39;),Path(&#39;01/frame_00004_pose.txt&#39;),Path(&#39;01/frame_00004_rgb.jpg&#39;),Path(&#39;01/frame_00005_pose.txt&#39;),Path(&#39;01/frame_00005_rgb.jpg&#39;),Path(&#39;01/frame_00006_pose.txt&#39;),Path(&#39;01/frame_00006_rgb.jpg&#39;),Path(&#39;01/frame_00007_pose.txt&#39;)...] . Inside the subdirectories, we have different frames, each of them come with an image (_rgb.jpg) and a pose file (_pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that converts an image filename to its associated pose file: . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) . Path(&#39;13/frame_00349_pose.txt&#39;) . Let&#39;s take a look at our first image: . im = PILImage.create(img_files[0]) im.shape . (480, 640) . im.to_thumb(160) . The Biwi dataset website used to explain the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren&#39;t important for our purposes, so we&#39;ll just show the function we use to extract the head center point: . cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) . This function returns the coordinates as a tensor of two items: . get_ctr(img_files[0]) . tensor([384.6370, 259.4787]) . We can pass this function to DataBlock as get_y, since it is responsible for labeling each item. We&#39;ll resize the images to half their input size, just to speed up training a bit. . One important point to note is that we should not just use a random splitter. The reason for this is that the same people appear in multiple images in this dataset, but we want to ensure that our model can generalize to people that it hasn&#39;t seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person&#39;s images. . The only other difference from the previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images: . biwi = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) . . Important: Points and Data Augmentation: We&#8217;re not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So, if you&#8217;re working with another library, you may need to disable data augmentation for these kinds of problems. . Before doing any modeling, we should look at our data to confirm it seems okay: . dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) . That&#39;s looking good! As well as looking at the batch visually, it&#39;s a good idea to also look at the underlying tensors (especially as a student; it will help clarify your understanding of what your model is really seeing): . xb,yb = dls.one_batch() xb.shape,yb.shape . (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) . Make sure that you understand why these are the shapes for our mini-batches. . Here&#39;s an example of one row from the dependent variable: . yb[0] . TensorPoint([[-0.3375, 0.2193]], device=&#39;cuda:6&#39;) . As you can see, we haven&#39;t had to use a separate image regression application; all we&#39;ve had to do is label the data, and tell fastai what kinds of data the independent and dependent variables represent. . It&#39;s the same for creating our Learner. We will use the same function as before, with one new parameter, and we will be ready to train our model. . Training a Model . As usual, we can use cnn_learner to create our Learner. Remember way back in &lt;&gt; how we used y_range to tell fastai the range of our targets? We&#39;ll do the same here (coordinates in fastai and PyTorch are always rescaled between -1 and +1):&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = cnn_learner(dls, resnet18, y_range=(-1,1)) . y_range is implemented in fastai using sigmoid_range, which is defined as: . def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo . This is set as the final layer of the model, if y_range is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range (lo,hi). . Here&#39;s what it looks like: . plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) . /home/jhoward/anaconda3/lib/python3.7/site-packages/fastbook/__init__.py:55: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . We didn&#39;t specify a loss function, which means we&#39;re getting whatever fastai chooses as the default. Let&#39;s see what it picked for us: . dls.loss_func . FlattenedLoss of MSELoss() . This makes sense, since when coordinates are used as the dependent variable, most of the time we&#39;re likely to be trying to predict something as close as possible; that&#39;s basically what MSELoss (mean squared error loss) does. If you want to use a different loss function, you can pass it to cnn_learner using the loss_func parameter. . Note also that we didn&#39;t specify any metrics. That&#39;s because the MSE is already a useful metric for this task (although it&#39;s probably more interpretable after we take the square root). . We can pick a good learning rate with the learning rate finder: . learn.lr_find() . SuggestedLRs(lr_min=0.005754399299621582, lr_steep=0.033113110810518265) . We&#39;ll try an LR of 1e-2: . lr = 1e-2 learn.fine_tune(3, lr) . epoch train_loss valid_loss time . 0 | 0.049630 | 0.007602 | 00:42 | . epoch train_loss valid_loss time . 0 | 0.008714 | 0.004291 | 00:53 | . 1 | 0.003213 | 0.000715 | 00:53 | . 2 | 0.001482 | 0.000036 | 00:53 | . Generally when we run this we get a loss of around 0.0001, which corresponds to an average coordinate prediction error of: . math.sqrt(0.0001) . 0.01 . This sounds very accurate! But it&#39;s important to take a look at our results with Learner.show_results. The left side are the actual (ground truth) coordinates and the right side are our model&#39;s predictions: . learn.show_results(ds_idx=1, nrows=3, figsize=(6,8)) . It&#39;s quite amazing that with just a few minutes of computation we&#39;ve created such an accurate key points model, and without any special domain-specific application. This is the power of building on flexible APIs, and using transfer learning! It&#39;s particularly striking that we&#39;ve been able to use transfer learning so effectively even between totally different tasks; our pretrained model was trained to do image classification, and we fine-tuned for image regression. . Conclusion . In problems that are at first glance completely different (single-label classification, multi-label classification, and regression), we end up using the same model with just different numbers of outputs. The loss function is the one thing that changes, which is why it&#39;s important to double-check that you are using the right loss function for your problem. . fastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want: . nn.CrossEntropyLoss for single-label classification | nn.BCEWithLogitsLoss for multi-label classification | nn.MSELoss for regression | . Questionnaire . How could multi-label classification improve the usability of the bear classifier? | How do we encode the dependent variable in a multi-label classification problem? | How do you access the rows and columns of a DataFrame as if it was a matrix? | How do you get a column by name from a DataFrame? | What is the difference between a Dataset and DataLoader? | What does a Datasets object normally contain? | What does a DataLoaders object normally contain? | What does lambda do in Python? | What are the methods to customize how the independent and dependent variables are created with the data block API? | Why is softmax not an appropriate output activation function when using a one hot encoded target? | Why is nll_loss not an appropriate loss function when using a one-hot-encoded target? | What is the difference between nn.BCELoss and nn.BCEWithLogitsLoss? | Why can&#39;t we use regular accuracy in a multi-label problem? | When is it okay to tune a hyperparameter on the validation set? | How is y_range implemented in fastai? (See if you can implement it yourself and test it without peeking!) | What is a regression problem? What loss function should you use for such a problem? | What do you need to do to make sure the fastai library applies the same data augmentation to your input images and your target point coordinates? | Further Research . Read a tutorial about Pandas DataFrames and experiment with a few methods that look interesting to you. See the book&#39;s website for recommended tutorials. | Retrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don&#39;t contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single-label dataset is impacted using multi-label classification. | &lt;/div&gt; .",
            "url": "https://pranath.github.io/fastbook/2021/02/19/06_multicat.html",
            "relUrl": "/2021/02/19/06_multicat.html",
            "date": " • Feb 19, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pranath.github.io/fastbook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pranath.github.io/fastbook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}