{
  
    
        "post0": {
            "title": "11 Data Munging with fastai's Mid-Level API",
            "content": "[[chapter_midlevel_data]] We have seen what Tokenizer and Numericalize do to a collection of texts, and how they&#39;re used inside the data block API, which handles those transforms for us directly using the TextBlock. But what if we want to only apply one of those transforms, either to see intermediate results or because we have already tokenized texts? More generally, what can we do when the data block API is not flexible enough to accommodate our particular use case? For this, we need to use fastai&#39;s mid-level API for processing data. The data block API is built on top of that layer, so it will allow you to do everything the data block API does, and much much more. . Going Deeper into fastai&#39;s Layered API . The fastai library is built on a layered API. In the very top layer there are applications that allow us to train a model in five lines of codes, as we saw in &lt;&gt;. In the case of creating DataLoaders for a text classifier, for instance, we used the line:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) . The factory method TextDataLoaders.from_folder is very convenient when your data is arranged the exact same way as the IMDb dataset, but in practice, that often won&#39;t be the case. The data block API offers more flexibility. As we saw in the last chapter, we can get the same result with: . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . But it&#39;s sometimes not flexible enough. For debugging purposes, for instance, we might need to apply just parts of the transforms that come with this data block. Or we might want to create a DataLoaders for some application that isn&#39;t directly supported by fastai. In this section, we&#39;ll dig into the pieces that are used inside fastai to implement the data block API. Understanding these will enable you to leverage the power and flexibility of this mid-tier API. . . Note: Mid-Level API: The mid-level API does not only contain functionality for creating DataLoaders. It also has the callback system, which allows us to customize the training loop any way we like, and the general optimizer. Both will be covered in &lt;&gt;. &lt;/div&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Transforms . When we studied tokenization and numericalization in the last chapter, we started by grabbing a bunch of texts: . files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) txts = L(o.open().read() for o in files[:2000]) . We then showed how to tokenize them with a Tokenizer: . tok = Tokenizer.from_folder(path) tok.setup(txts) toks = txts.map(tok) toks[0] . (#374) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;well&#39;,&#39;,&#39;,&#39;&#34;&#39;,&#39;cube&#39;,&#39;&#34;&#39;,&#39;(&#39;,&#39;1997&#39;,&#39;)&#39;...] . and how to numericalize, including automatically creating the vocab for our corpus: . num = Numericalize() num.setup(toks) nums = toks.map(num) nums[0][:10] . tensor([ 2, 8, 76, 10, 23, 3112, 23, 34, 3113, 33]) . The classes also have a decode method. For instance, Numericalize.decode gives us back the string tokens: . nums_dec = num.decode(nums[0][:10]); nums_dec . (#10) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;well&#39;,&#39;,&#39;,&#39;&#34;&#39;,&#39;cube&#39;,&#39;&#34;&#39;,&#39;(&#39;,&#39;1997&#39;,&#39;)&#39;] . and Tokenizer.decode turns this back into a single string (it may not, however, be exactly the same as the original string; this depends on whether the tokenizer is reversible, which the default word tokenizer is not at the time we&#39;re writing this book): . tok.decode(nums_dec) . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 )&#39; . decode is used by fastai&#39;s show_batch and show_results, as well as some other inference methods, to convert predictions and mini-batches into a human-understandable representation. . For each of tok or num in the preceding example, we created an object, called the setup method (which trains the tokenizer if needed for tok and creates the vocab for num), applied it to our raw texts (by calling the object as a function), and then finally decoded the result back to an understandable representation. These steps are needed for most data preprocessing tasks, so fastai provides a class that encapsulates them. This is the Transform class. Both Tokenize and Numericalize are Transforms. . In general, a Transform is an object that behaves like a function and has an optional setup method that will initialize some inner state (like the vocab inside num) and an optional decode that will reverse the function (this reversal may not be perfect, as we saw with tok). . A good example of decode is found in the Normalize transform that we saw in &lt;&gt;: to be able to plot the images its decode method undoes the normalization (i.e., it multiplies by the standard deviation and adds back the mean). On the other hand, data augmentation transforms do not have a decode method, since we want to show the effects on images to make sure the data augmentation is working as we want.&lt;/p&gt; A special behavior of Transforms is that they always get applied over tuples. In general, our data is always a tuple (input,target) (sometimes with more than one input or more than one target). When applying a transform on an item like this, such as Resize, we don&#39;t want to resize the tuple as a whole; instead, we want to resize the input (if applicable) and the target (if applicable) separately. It&#39;s the same for batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target. . We can see this behavior if we pass a tuple of texts to tok: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; tok((txts[0], txts[1])) . ((#374) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;well&#39;,&#39;,&#39;,&#39;&#34;&#39;,&#39;cube&#39;,&#39;&#34;&#39;,&#39;(&#39;,&#39;1997&#39;,&#39;)&#39;...], (#207) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;conrad&#39;,&#39;xxmaj&#39;,&#39;hall&#39;,&#39;went&#39;,&#39;out&#39;,&#39;with&#39;,&#39;a&#39;,&#39;bang&#39;...]) . Writing Your Own Transform . If you want to write a custom transform to apply to your data, the easiest way is to write a function. As you can see in this example, a Transform will only be applied to a matching type, if a type is provided (otherwise it will always be applied). In the following code, the :int in the function signature means that f only gets applied to ints. That&#39;s why tfm(2.0) returns 2.0, but tfm(2) returns 3 here: . def f(x:int): return x+1 tfm = Transform(f) tfm(2),tfm(2.0) . (3, 2.0) . Here, f is converted to a Transform with no setup and no decode method. . Python has a special syntax for passing a function (like f) to another function (or something that behaves like a function, known as a callable in Python), called a decorator. A decorator is used by prepending a callable with @ and placing it before a function definition (there are lots of good online tutorials about Python decorators, so take a look at one if this is a new concept for you). The following is identical to the previous code: . @Transform def f(x:int): return x+1 f(2),f(2.0) . (3, 2.0) . If you need either setup or decode, you will need to subclass Transform to implement the actual encoding behavior in encodes, then (optionally), the setup behavior in setups and the decoding behavior in decodes: . class NormalizeMean(Transform): def setups(self, items): self.mean = sum(items)/len(items) def encodes(self, x): return x-self.mean def decodes(self, x): return x+self.mean . Here, NormalizeMean will initialize some state during the setup (the mean of all elements passed), then the transformation is to subtract that mean. For decoding purposes, we implement the reverse of that transformation by adding the mean. Here is an example of NormalizeMean in action: . tfm = NormalizeMean() tfm.setup([1,2,3,4,5]) start = 2 y = tfm(start) z = tfm.decode(y) tfm.mean,y,z . (3.0, -1.0, 2.0) . Note that the method called and the method implemented are different, for each of these methods: . asciidoc [options=&quot;header&quot;] |====== | Class | To call | To implement | `nn.Module` (PyTorch) | `()` (i.e., call as function) | `forward` | `Transform` | `()` | `encodes` | `Transform` | `decode()` | `decodes` | `Transform` | `setup()` | `setups` |====== . So, for instance, you would never call setups directly, but instead would call setup. The reason for this is that setup does some work before and after calling setups for you. To learn more about Transforms and how you can use them to implement different behavior depending on the type of the input, be sure to check the tutorials in the fastai docs. . Pipeline . To compose several transforms together, fastai provides the Pipeline class. We define a Pipeline by passing it a list of Transforms; it will then compose the transforms inside it. When you call Pipeline on an object, it will automatically call the transforms inside, in order: . tfms = Pipeline([tok, num]) t = tfms(txts[0]); t[:20] . tensor([ 2, 8, 76, 10, 23, 3112, 23, 34, 3113, 33, 10, 8, 4477, 22, 88, 32, 10, 27, 42, 14]) . And you can call decode on the result of your encoding, to get back something you can display and analyze: . tfms.decode(t)[:100] . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesti&#39; . The only part that doesn&#39;t work the same way as in Transform is the setup. To properly set up a Pipeline of Transforms on some data, you need to use a TfmdLists. . TfmdLists and Datasets: Transformed Collections . Your data is usually a set of raw items (like filenames, or rows in a DataFrame) to which you want to apply a succession of transformations. We just saw that a succession of transformations is represented by a Pipeline in fastai. The class that groups together this Pipeline with your raw items is called TfmdLists. . TfmdLists . Here is the short way of doing the transformation we saw in the previous section: . tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize]) . At initialization, the TfmdLists will automatically call the setup method of each Transform in order, providing them not with the raw items but the items transformed by all the previous Transforms in order. We can get the result of our Pipeline on any raw element just by indexing into the TfmdLists: . t = tls[0]; t[:20] . tensor([ 2, 8, 91, 11, 22, 5793, 22, 37, 4910, 34, 11, 8, 13042, 23, 107, 30, 11, 25, 44, 14]) . And the TfmdLists knows how to decode for show purposes: . tls.decode(t)[:100] . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesti&#39; . In fact, it even has a show method: . tls.show(t) . xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesting and tricky ideas that xxmaj i &#39;ve ever seen when talking about movies . xxmaj they had just one scenery , a bunch of actors and a plot . xxmaj so , what made it so special were all the effective direction , great dialogs and a bizarre condition that characters had to deal like rats in a labyrinth . xxmaj his second movie , &#34; cypher &#34; ( 2002 ) , was all about its story , but it was n&#39;t so good as &#34; cube &#34; but here are the characters being tested like rats again . &#34; nothing &#34; is something very interesting and gets xxmaj vincenzo coming back to his &#39; cube days &#39; , locking the characters once again in a very different space with no time once more playing with the characters like playing with rats in an experience room . xxmaj but instead of a thriller sci - fi ( even some of the promotional teasers and trailers erroneous seemed like that ) , &#34; nothing &#34; is a loose and light comedy that for sure can be called a modern satire about our society and also about the intolerant world we &#39;re living . xxmaj once again xxmaj xxunk amaze us with a great idea into a so small kind of thing . 2 actors and a blinding white scenario , that &#39;s all you got most part of time and you do n&#39;t need more than that . xxmaj while &#34; cube &#34; is a claustrophobic experience and &#34; cypher &#34; confusing , &#34; nothing &#34; is completely the opposite but at the same time also desperate . xxmaj this movie proves once again that a smart idea means much more than just a millionaire budget . xxmaj of course that the movie fails sometimes , but its prime idea means a lot and offsets any flaws . xxmaj there &#39;s nothing more to be said about this movie because everything is a brilliant surprise and a totally different experience that i had in movies since &#34; cube &#34; . . The TfmdLists is named with an &quot;s&quot; because it can handle a training and a validation set with a splits argument. You just need to pass the indices of which elements are in the training set, and which are in the validation set: . cut = int(len(files)*0.8) splits = [list(range(cut)), list(range(cut,len(files)))] tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], splits=splits) . You can then access them through the train and valid attributes: . tls.valid[0][:20] . tensor([ 2, 8, 20, 30, 87, 510, 1570, 12, 408, 379, 4196, 10, 8, 20, 30, 16, 13, 12216, 202, 509]) . If you have manually written a Transform that performs all of your preprocessing at once, turning raw items into a tuple with inputs and targets, then TfmdLists is the class you need. You can directly convert it to a DataLoaders object with the dataloaders method. This is what we will do in our Siamese example later in this chapter. . In general, though, you will have two (or more) parallel pipelines of transforms: one for processing your raw items into inputs and one to process your raw items into targets. For instance, here, the pipeline we defined only processes the raw text into inputs. If we want to do text classification, we also have to process the labels into targets. . For this we need to do two things. First we take the label name from the parent folder. There is a function, parent_label, for this: . lbls = files.map(parent_label) lbls . (#50000) [&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;...] . Then we need a Transform that will grab the unique items and build a vocab with them during setup, then transform the string labels into integers when called. fastai provides this for us; it&#39;s called Categorize: . cat = Categorize() cat.setup(lbls) cat.vocab, cat(lbls[0]) . ((#2) [&#39;neg&#39;,&#39;pos&#39;], TensorCategory(1)) . To do the whole setup automatically on our list of files, we can create a TfmdLists as before: . tls_y = TfmdLists(files, [parent_label, Categorize()]) tls_y[0] . TensorCategory(1) . But then we end up with two separate objects for our inputs and targets, which is not what we want. This is where Datasets comes to the rescue. . Datasets . Datasets will apply two (or more) pipelines in parallel to the same raw object and build a tuple with the result. Like TfmdLists, it will automatically do the setup for us, and when we index into a Datasets, it will return us a tuple with the results of each pipeline: . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms]) x,y = dsets[0] x[:20],y . Like a TfmdLists, we can pass along splits to a Datasets to split our data between training and validation sets: . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms], splits=splits) x,y = dsets.valid[0] x[:20],y . (tensor([ 2, 8, 20, 30, 87, 510, 1570, 12, 408, 379, 4196, 10, 8, 20, 30, 16, 13, 12216, 202, 509]), TensorCategory(0)) . It can also decode any processed tuple or show it directly: . t = dsets.valid[0] dsets.decode(t) . (&#39;xxbos xxmaj this movie had horrible lighting and terrible camera movements . xxmaj this movie is a jumpy horror flick with no meaning at all . xxmaj the slashes are totally fake looking . xxmaj it looks like some 17 year - old idiot wrote this movie and a 10 year old kid shot it . xxmaj with the worst acting you can ever find . xxmaj people are tired of knives . xxmaj at least move on to guns or fire . xxmaj it has almost exact lines from &#34; when a xxmaj stranger xxmaj calls &#34; . xxmaj with gruesome killings , only crazy people would enjoy this movie . xxmaj it is obvious the writer does n &#39;t have kids or even care for them . i mean at show some mercy . xxmaj just to sum it up , this movie is a &#34; b &#34; movie and it sucked . xxmaj just for your own sake , do n &#39;t even think about wasting your time watching this crappy movie .&#39;, &#39;neg&#39;) . The last step is to convert our Datasets object to a DataLoaders, which can be done with the dataloaders method. Here we need to pass along a special argument to take care of the padding problem (as we saw in the last chapter). This needs to happen just before we batch the elements, so we pass it to before_batch: . dls = dsets.dataloaders(bs=64, before_batch=pad_input) . dataloaders directly calls DataLoader on each subset of our Datasets. fastai&#39;s DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. It has a lot of points of customization, but the most important ones that you should know are: . after_item:: Applied on each item after grabbing it inside the dataset. This is the equivalent of item_tfms in DataBlock. | before_batch:: Applied on the list of items before they are collated. This is the ideal place to pad items to the same size. | after_batch:: Applied on the batch as a whole after its construction. This is the equivalent of batch_tfms in DataBlock. | . As a conclusion, here is the full code necessary to prepare the data for text classification: . tfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]] files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) splits = GrandparentSplitter(valid_name=&#39;test&#39;)(files) dsets = Datasets(files, tfms, splits=splits) dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input) . The two differences from the previous code are the use of GrandparentSplitter to split our training and validation data, and the dl_type argument. This is to tell dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches. . This does the exact same thing as our previous DataBlock: . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . But now, you know how to customize every single piece of it! . Let&#39;s practice what we just learned about this mid-level API for data preprocessing, using a computer vision example now. . Applying the Mid-Level Data API: SiamesePair . A Siamese model takes two images and has to determine if they are of the same class or not. For this example, we will use the Pet dataset again and prepare the data for a model that will have to predict if two images of pets are of the same breed or not. We will explain here how to prepare the data for such a model, then we will train that model in &lt;&gt;.&lt;/p&gt; First things first, let&#39;s get the images in our dataset: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.vision.all import * path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) . If we didn&#39;t care about showing our objects at all, we could directly create one transform to completely preprocess that list of files. We will want to look at those images though, so we need to create a custom type. When you call the show method on a TfmdLists or a Datasets object, it will decode items until it reaches a type that contains a show method and use it to show the object. That show method gets passed a ctx, which could be a matplotlib axis for images, or a row of a DataFrame for texts. . Here we create a SiameseImage object that subclasses fastuple and is intended to contain three things: two images, and a Boolean that&#39;s True if the images are of the same breed. We also implement the special show method, such that it concatenates the two images with a black line in the middle. Don&#39;t worry too much about the part that is in the if test (which is to show the SiameseImage when the images are Python images, not tensors); the important part is in the last three lines: . class SiameseImage(fastuple): def show(self, ctx=None, **kwargs): img1,img2,same_breed = self if not isinstance(img1, Tensor): if img2.size != img1.size: img2 = img2.resize(img1.size) t1,t2 = tensor(img1),tensor(img2) t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1) else: t1,t2 = img1,img2 line = t1.new_zeros(t1.shape[0], t1.shape[1], 10) return show_image(torch.cat([t1,line,t2], dim=2), title=same_breed, ctx=ctx) . Let&#39;s create a first SiameseImage and check our show method works: . img = PILImage.create(files[0]) s = SiameseImage(img, img, True) s.show(); . We can also try with a second image that&#39;s not from the same class: . img1 = PILImage.create(files[1]) s1 = SiameseImage(img, img1, False) s1.show(); . The important thing with transforms that we saw before is that they dispatch over tuples or their subclasses. That&#39;s precisely why we chose to subclass fastuple in this instance—this way we can apply any transform that works on images to our SiameseImage and it will be applied on each image in the tuple: . s2 = Resize(224)(s1) s2.show(); . Here the Resize transform is applied to each of the two images, but not the Boolean flag. Even if we have a custom type, we can thus benefit from all the data augmentation transforms inside the library. . We are now ready to build the Transform that we will use to get our data ready for a Siamese model. First, we will need a function to determine the classes of all our images: . def label_func(fname): return re.match(r&#39;^(.*)_ d+.jpg$&#39;, fname.name).groups()[0] . For each image our tranform will, with a probability of 0.5, draw an image from the same class and return a SiameseImage with a true label, or draw an image from another class and return a SiameseImage with a false label. This is all done in the private _draw function. There is one difference between the training and validation sets, which is why the transform needs to be initialized with the splits: on the training set we will make that random pick each time we read an image, whereas on the validation set we make this random pick once and for all at initialization. This way, we get more varied samples during training, but always the same validation set: . class SiameseTransform(Transform): def __init__(self, files, label_func, splits): self.labels = files.map(label_func).unique() self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels} self.label_func = label_func self.valid = {f: self._draw(f) for f in files[splits[1]]} def encodes(self, f): f2,t = self.valid.get(f, self._draw(f)) img1,img2 = PILImage.create(f),PILImage.create(f2) return SiameseImage(img1, img2, t) def _draw(self, f): same = random.random() &lt; 0.5 cls = self.label_func(f) if not same: cls = random.choice(L(l for l in self.labels if l != cls)) return random.choice(self.lbl2files[cls]),same . We can then create our main transform: . splits = RandomSplitter()(files) tfm = SiameseTransform(files, label_func, splits) tfm(files[0]).show(); . In the mid-level API for data collection we have two objects that can help us apply transforms on a set of items, TfmdLists and Datasets. If you remember what we have just seen, one applies a Pipeline of transforms and the other applies several Pipelines of transforms in parallel, to build tuples. Here, our main transform already builds the tuples, so we use TfmdLists: . tls = TfmdLists(files, tfm, splits=splits) show_at(tls.valid, 0); . And we can finally get our data in DataLoaders by calling the dataloaders method. One thing to be careful of here is that this method does not take item_tfms and batch_tfms like a DataBlock. The fastai DataLoader has several hooks that are named after events; here what we apply on the items after they are grabbed is called after_item, and what we apply on the batch once it&#39;s built is called after_batch: . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . Note that we need to pass more transforms than usual—that&#39;s because the data block API usually adds them automatically: . ToTensor is the one that converts images to tensors (again, it&#39;s applied on every part of the tuple). | IntToFloatTensor converts the tensor of images containing integers from 0 to 255 to a tensor of floats, and divides by 255 to make the values between 0 and 1. | . We can now train a model using this DataLoaders. It will need a bit more customization than the usual model provided by cnn_learner since it has to take two images instead of one, but we will see how to create such a model and train it in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Conclusion . fastai provides a layered API. It takes one line of code to grab the data when it&#39;s in one of the usual settings, making it easy for beginners to focus on training a model without spending too much time assembling the data. Then, the high-level data block API gives you more flexibility by allowing you to mix and match some building blocks. Underneath it, the mid-level API gives you greater flexibility to apply any transformations on your items. In your real-world problems, this is probably what you will need to use, and we hope it makes the step of data-munging as easy as possible. . Questionnaire . Why do we say that fastai has a &quot;layered&quot; API? What does it mean? | Why does a Transform have a decode method? What does it do? | Why does a Transform have a setup method? What does it do? | How does a Transform work when called on a tuple? | Which methods do you need to implement when writing your own Transform? | Write a Normalize transform that fully normalizes items (subtract the mean and divide by the standard deviation of the dataset), and that can decode that behavior. Try not to peek! | Write a Transform that does the numericalization of tokenized texts (it should set its vocab automatically from the dataset seen and have a decode method). Look at the source code of fastai if you need help. | What is a Pipeline? | What is a TfmdLists? | What is a Datasets? How is it different from a TfmdLists? | Why are TfmdLists and Datasets named with an &quot;s&quot;? | How can you build a DataLoaders from a TfmdLists or a Datasets? | How do you pass item_tfms and batch_tfms when building a DataLoaders from a TfmdLists or a Datasets? | What do you need to do when you want to have your custom items work with methods like show_batch or show_results? | Why can we easily apply fastai data augmentation transforms to the SiamesePair we built? | Further Research . Use the mid-level API to prepare the data in DataLoaders on your own datasets. Try this with the Pet dataset and the Adult dataset from Chapter 1. | Look at the Siamese tutorial in the fastai documentation to learn how to customize the behavior of show_batch and show_results for new type of items. Implement it in your own project. | Understanding fastai&#39;s Applications: Wrap Up . Congratulations—you&#39;ve completed all of the chapters in this book that cover the key practical parts of training models and using deep learning! You know how to use all of fastai&#39;s built-in applications, and how to customize them using the data block API and loss functions. You even know how to create a neural network from scratch, and train it! (And hopefully you now know some of the questions to ask to make sure your creations help improve society too.) . The knowledge you already have is enough to create full working prototypes of many types of neural network applications. More importantly, it will help you understand the capabilities and limitations of deep learning models, and how to design a system that&#39;s well adapted to them. . In the rest of this book we will be pulling apart those applications, piece by piece, to understand the foundations they are built on. This is important knowledge for a deep learning practitioner, because it is what allows you to inspect and debug models that you build and create new applications that are customized for your particular projects. . &lt;/div&gt; . .",
            "url": "https://pranath.github.io/fastbook/2021/02/19/11_midlevel_data.html",
            "relUrl": "/2021/02/19/11_midlevel_data.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "6 Other Computer Vision Problems",
            "content": "[[chapter_multicat]] In the previous chapter you learned some important practical techniques for training models in practice. Considerations like selecting learning rates and the number of epochs are very important to getting good results. . In this chapter we are going to look at two other types of computer vision problems: multi-label classification and regression. The first one is when you want to predict more than one label per image (or sometimes none at all), and the second is when your labels are one or several numbers—a quantity instead of a category. . In the process will study more deeply the output activations, targets, and loss functions in deep learning models. . Multi-Label Classification . Multi-label classification refers to the problem of identifying the categories of objects in images that may not contain exactly one type of object. There may be more than one kind of object, or there may be no objects at all in the classes that you are looking for. . For instance, this would have been a great approach for our bear classifier. One problem with the bear classifier that we rolled out in &lt;&gt; was that if a user uploaded something that wasn&#39;t any kind of bear, the model would still say it was either a grizzly, black, or teddy bear—it had no ability to predict &quot;not a bear at all.&quot; In fact, after we have completed this chapter, it would be a great exercise for you to go back to your image classifier application, and try to retrain it using the multi-label technique, then test it by passing in an image that is not of any of your recognized classes.&lt;/p&gt; In practice, we have not seen many examples of people training multi-label classifiers for this purpose—but we very often see both users and developers complaining about this problem. It appears that this simple solution is not at all widely understood or appreciated! Because in practice it is probably more common to have some images with zero matches or more than one match, we should probably expect in practice that multi-label classifiers are more widely applicable than single-label classifiers. . First, let&#39;s see what a multi-label dataset looks like, then we&#39;ll explain how to get it ready for our model. You&#39;ll see that the architecture of the model does not change from the last chapter; only the loss function does. Let&#39;s start with the data. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The Data . For our example we are going to use the PASCAL dataset, which can have more than one kind of classified object per image. . We begin by downloading and extracting the dataset as per usual: . from fastai.vision.all import * path = untar_data(URLs.PASCAL_2007) . This dataset is different from the ones we have seen before, in that it is not structured by filename or folder but instead comes with a CSV (comma-separated values) file telling us what labels to use for each image. We can inspect the CSV file by reading it into a Pandas DataFrame: . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . As you can see, the list of categories in each image is shown as a space-delimited string. . Sidebar: Pandas and DataFrames . No, it’s not actually a panda! Pandas is a Python library that is used to manipulate and analyze tabular and time series data. The main class is DataFrame, which represents a table of rows and columns. You can get a DataFrame from a CSV file, a database table, Python dictionaries, and many other sources. In Jupyter, a DataFrame is output as a formatted table, as shown here. . You can access rows and columns of a DataFrame with the iloc property, as if it were a matrix: . df.iloc[:,0] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . df.iloc[0,:] # Trailing :s are always optional (in numpy, pytorch, pandas, etc.), # so this is equivalent: df.iloc[0] . fname 000005.jpg labels chair is_valid True Name: 0, dtype: object . You can also grab a column by name by indexing into a DataFrame directly: . df[&#39;fname&#39;] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . You can create new columns and do calculations using columns: . tmp_df = pd.DataFrame({&#39;a&#39;:[1,2], &#39;b&#39;:[3,4]}) tmp_df . a b . 0 1 | 3 | . 1 2 | 4 | . tmp_df[&#39;c&#39;] = tmp_df[&#39;a&#39;]+tmp_df[&#39;b&#39;] tmp_df . a b c . 0 1 | 3 | 4 | . 1 2 | 4 | 6 | . Pandas is a fast and flexible library, and an important part of every data scientist’s Python toolbox. Unfortunately, its API can be rather confusing and surprising, so it takes a while to get familiar with it. If you haven’t used Pandas before, we’d suggest going through a tutorial; we are particularly fond of the book Python for Data Analysis by Wes McKinney, the creator of Pandas (O&#39;Reilly). It also covers other important libraries like matplotlib and numpy. We will try to briefly describe Pandas functionality we use as we come across it, but will not go into the level of detail of McKinney’s book. . End sidebar . Now that we have seen what the data looks like, let&#39;s make it ready for model training. . Constructing a DataBlock . How do we convert from a DataFrame object to a DataLoaders object? We generally suggest using the data block API for creating a DataLoaders object, where possible, since it provides a good mix of flexibility and simplicity. Here we will show you the steps that we take to use the data blocks API to construct a DataLoaders object in practice, using this dataset as an example. . As we have seen, PyTorch and fastai have two main classes for representing and accessing a training set or validation set: . Dataset:: A collection that returns a tuple of your independent and dependent variable for a single item | DataLoader:: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables | . On top of these, fastai provides two classes for bringing your training and validation sets together: . Datasets:: An object that contains a training Dataset and a validation Dataset | DataLoaders:: An object that contains a training DataLoader and a validation DataLoader | . Since a DataLoader builds on top of a Dataset and adds additional functionality to it (collating multiple items into a mini-batch), it’s often easiest to start by creating and testing Datasets, and then look at DataLoaders after that’s working. . When we create a DataBlock, we build up gradually, step by step, and use the notebook to check our data along the way. This is a great way to make sure that you maintain momentum as you are coding, and that you keep an eye out for any problems. It’s easy to debug, because you know that if a problem arises, it is in the line of code you just typed! . Let’s start with the simplest case, which is a data block created with no parameters: . dblock = DataBlock() . We can create a Datasets object from this. The only thing needed is a source—in this case, our DataFrame: . dsets = dblock.datasets(df) . This contains a train and a valid dataset, which we can index into: . len(dsets.train),len(dsets.valid) . (4009, 1002) . x,y = dsets.train[0] x,y . (fname 008663.jpg labels car person is_valid False Name: 4346, dtype: object, fname 008663.jpg labels car person is_valid False Name: 4346, dtype: object) . As you can see, this simply returns a row of the DataFrame, twice. This is because by default, the data block assumes we have two things: input and target. We are going to need to grab the appropriate fields from the DataFrame, which we can do by passing get_x and get_y functions: . x[&#39;fname&#39;] . &#39;008663.jpg&#39; . dblock = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets = dblock.datasets(df) dsets.train[0] . (&#39;005620.jpg&#39;, &#39;aeroplane&#39;) . As you can see, rather than defining a function in the usual way, we are using Python’s lambda keyword. This is just a shortcut for defining and then referring to a function. The following more verbose approach is identical: . def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;] dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (&#39;002549.jpg&#39;, &#39;tvmonitor&#39;) . Lambda functions are great for quickly iterating, but they are not compatible with serialization, so we advise you to use the more verbose approach if you want to export your Learner after training (lambdas are fine if you are just experimenting). . We can see that the independent variable will need to be converted into a complete path, so that we can open it as an image, and the dependent variable will need to be split on the space character (which is the default for Python’s split function) so that it becomes a list: . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (Path(&#39;/home/jhoward/.fastai/data/pascal_2007/train/002844.jpg&#39;), [&#39;train&#39;]) . To actually open the image and do the conversion to tensors, we will need to use a set of transforms; block types will provide us with those. We can use the same block types that we have used previously, with one exception: the ImageBlock will work fine again, because we have a path that points to a valid image, but the CategoryBlock is not going to work. The problem is that block returns a single integer, but we need to be able to have multiple labels for each item. To solve this, we use a MultiCategoryBlock. This type of block expects to receive a list of strings, as we have in this case, so let’s test it out: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x375, TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])) . As you can see, our list of categories is not encoded in the same way that it was for the regular CategoryBlock. In that case, we had a single integer representing which category was present, based on its location in our vocab. In this case, however, we instead have a list of zeros, with a one in any position where that category is present. For example, if there is a one in the second and fourth positions, then that means that vocab items two and four are present in this image. This is known as one-hot encoding. The reason we can’t easily just use a list of category indices is that each list would be a different length, and PyTorch requires tensors, where everything has to be the same length. . jargon:One-hot encoding: Using a vector of zeros, with a one in each location that is represented in the data, to encode a list of integers. . Let’s check what the categories represent for this example (we are using the convenient torch.where function, which tells us all of the indices where our condition is true or false): . idxs = torch.where(dsets.train[0][1]==1.)[0] dsets.train.vocab[idxs] . (#1) [&#39;dog&#39;] . With NumPy arrays, PyTorch tensors, and fastai’s L class, we can index directly using a list or vector, which makes a lot of code (such as this example) much clearer and more concise. . We have ignored the column is_valid up until now, which means that DataBlock has been using a random split by default. To explicitly choose the elements of our validation set, we need to write a function and pass it to splitter (or use one of fastai&#39;s predefined functions or classes). It will take the items (here our whole DataFrame) and must return two (or more) lists of integers: . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() valid = df.index[df[&#39;is_valid&#39;]].tolist() return train,valid dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . As we have discussed, a DataLoader collates the items from a Dataset into a mini-batch. This is a tuple of tensors, where each tensor simply stacks the items from that location in the Dataset item. . Now that we have confirmed that the individual items look okay, there&#39;s one more step we need to ensure we can create our DataLoaders, which is to ensure that every item is of the same size. To do this, we can use RandomResizedCrop: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(df) . And now we can display a sample of our data: . dls.show_batch(nrows=1, ncols=3) . Remember that if anything goes wrong when you create your DataLoaders from your DataBlock, or if you want to view exactly what happens with your DataBlock, you can use the summary method we presented in the last chapter. . Our data is now ready for training a model. As we will see, nothing is going to change when we create our Learner, but behind the scenes, the fastai library will pick a new loss function for us: binary cross-entropy. . Binary Cross-Entropy . Now we&#39;ll create our Learner. We saw in &lt;&gt; that a Learner object contains four main things: the model, a DataLoaders object, an Optimizer, and the loss function to use. We already have our DataLoaders, we can leverage fastai&#39;s resnet models (which we&#39;ll learn how to create from scratch later), and we know how to create an SGD optimizer. So let&#39;s focus on ensuring we have a suitable loss function. To do this, let&#39;s use cnn_learner to create a Learner, so we can look at its activations:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = cnn_learner(dls, resnet18) . We also saw that the model in a Learner is generally an object of a class inheriting from nn.Module, and that we can call it using parentheses and it will return the activations of a model. You should pass it your independent variable, as a mini-batch. We can try it out by grabbing a mini batch from our DataLoader and then passing it to the model: . x,y = to_cpu(dls.train.one_batch()) activs = learn.model(x) activs.shape . torch.Size([64, 20]) . Think about why activs has this shape—we have a batch size of 64, and we need to calculate the probability of each of 20 categories. Here’s what one of those activations looks like: . activs[0] . TensorImage([ 0.7476, -1.1988, 4.5421, -1.5915, -0.6749, 0.0343, -2.4930, -0.8330, -0.3817, -1.4876, -0.1683, 2.1547, -3.4151, -1.1743, 0.1530, -1.6801, -2.3067, 0.7063, -1.3358, -0.3715], grad_fn=&lt;AliasBackward&gt;) . . Note: Getting Model Activations: Knowing how to manually get a mini-batch and pass it into a model, and look at the activations and loss, is really important for debugging your model. It is also very helpful for learning, so that you can see exactly what is going on. . They aren’t yet scaled to between 0 and 1, but we learned how to do that in &lt;&gt;, using the sigmoid function. We also saw how to calculate a loss based on this—this is our loss function from &lt;&gt;, with the addition of log as discussed in the last chapter:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, 1-inputs, inputs).log().mean() . Note that because we have a one-hot-encoded dependent variable, we can&#39;t directly use nll_loss or softmax (and therefore we can&#39;t use cross_entropy): . softmax, as we saw, requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (due to the use of exp); however, we may well have multiple objects that we&#39;re confident appear in an image, so restricting the maximum sum of activations to 1 is not a good idea. By the same reasoning, we may want the sum to be less than 1, if we don&#39;t think any of the categories appear in an image. | nll_loss, as we saw, returns the value of just one activation: the single activation corresponding with the single label for an item. This doesn&#39;t make sense when we have multiple labels. | . On the other hand, the binary_cross_entropy function, which is just mnist_loss along with log, provides just what we need, thanks to the magic of PyTorch&#39;s elementwise operations. Each activation will be compared to each target for each column, so we don&#39;t have to do anything to make this function work for multiple columns. . j:One of the things I really like about working with libraries like PyTorch, with broadcasting and elementwise operations, is that quite frequently I find I can write code that works equally well for a single item or a batch of items, without changes. binary_cross_entropy is a great example of this. By using these operations, we don&#39;t have to write loops ourselves, and can rely on PyTorch to do the looping we need as appropriate for the rank of the tensors we&#39;re working with. . PyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names! . F.binary_cross_entropy and its module equivalent nn.BCELoss calculate cross-entropy on a one-hot-encoded target, but do not include the initial sigmoid. Normally for one-hot-encoded targets you&#39;ll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross-entropy in a single function, as in the preceding example. . The equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, is F.nll_loss or nn.NLLLoss for the version without the initial softmax, and F.cross_entropy or nn.CrossEntropyLoss for the version with the initial softmax. . Since we have a one-hot-encoded target, we will use BCEWithLogitsLoss: . loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs, y) loss . TensorImage(1.0342, grad_fn=&lt;AliasBackward&gt;) . We don&#39;t actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders has multiple category labels, so it will use nn.BCEWithLogitsLoss by default. . One change compared to the last chapter is the metric we use: because this is a multilabel problem, we can&#39;t use the accuracy function. Why is that? Well, accuracy was comparing our outputs to our targets like so: . def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred = inp.argmax(dim=axis) return (pred == targ).float().mean() . The class predicted was the one with the highest activation (this is what argmax does). Here it doesn&#39;t work because we could have more than one prediction on a single image. After applying the sigmoid to our activations (to make them between 0 and 1), we need to decide which ones are 0s and which ones are 1s by picking a threshold. Each value above the threshold will be considered as a 1, and each value lower than the threshold will be considered a 0: . def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() . If we pass accuracy_multi directly as a metric, it will use the default value for threshold, which is 0.5. We might want to adjust that default and create a new version of accuracy_multi that has a different default. To help with this, there is a function in Python called partial. It allows us to bind a function with some arguments or keyword arguments, making a new version of that function that, whenever it is called, always includes those arguments. For instance, here is a simple function taking two arguments: . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) . (&#39;Hello Jeremy.&#39;, &#39;Ahoy! Jeremy.&#39;) . We can switch to a French version of that function by using partial: . f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) . (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . We can now train our model. Let&#39;s try setting the accuracy threshold to 0.2 for our metric: . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.942663 | 0.703737 | 0.233307 | 00:08 | . 1 | 0.821548 | 0.550827 | 0.295319 | 00:08 | . 2 | 0.604189 | 0.202585 | 0.816474 | 00:08 | . 3 | 0.359258 | 0.123299 | 0.944283 | 00:08 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.135746 | 0.123404 | 0.944442 | 00:09 | . 1 | 0.118443 | 0.107534 | 0.951255 | 00:09 | . 2 | 0.098525 | 0.104778 | 0.951554 | 00:10 | . Picking a threshold is important. If you pick a threshold that&#39;s too low, you&#39;ll often be failing to select correctly labeled objects. We can see this by changing our metric, and then calling validate, which returns the validation loss and metrics: . learn.metrics = partial(accuracy_multi, thresh=0.1) learn.validate() . (#2) [0.10477833449840546,0.9314740300178528] . If you pick a threshold that&#39;s too high, you&#39;ll only be selecting the objects for which your model is very confident: . learn.metrics = partial(accuracy_multi, thresh=0.99) learn.validate() . (#2) [0.10477833449840546,0.9429482221603394] . We can find the best threshold by trying a few levels and seeing what works best. This is much faster if we just grab the predictions once: . preds,targs = learn.get_preds() . Then we can call the metric directly. Note that by default get_preds applies the output activation function (sigmoid, in this case) for us, so we&#39;ll need to tell accuracy_multi to not apply it: . accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) . TensorImage(0.9567) . We can now use this approach to find the best threshold level: . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . In this case, we&#39;re using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be overfitting to the validation set, since we&#39;re trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we&#39;re clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don&#39;t try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it&#39;s fine to do this). . This concludes the part of this chapter dedicated to multi-label classification. Next, we&#39;ll take a look at a regression problem. . Regression . It&#39;s easy to think of deep learning models as being classified into domains, like computer vision, NLP, and so forth. And indeed, that&#39;s how fastai classifies its applications—largely because that&#39;s how most people are used to thinking of things. . But really, that&#39;s hiding a more interesting and deeper perspective. A model is defined by its independent and dependent variables, along with its loss function. That means that there&#39;s really a far wider array of models than just the simple domain-based split. Perhaps we have an independent variable that&#39;s an image, and a dependent that&#39;s text (e.g., generating a caption from an image); or perhaps we have an independent variable that&#39;s text and dependent that&#39;s an image (e.g., generating an image from a caption—which is actually possible for deep learning to do!); or perhaps we&#39;ve got images, texts, and tabular data as independent variables, and we&#39;re trying to predict product purchases... the possibilities really are endless. . To be able to move beyond fixed applications, to crafting your own novel solutions to novel problems, it helps to really understand the data block API (and maybe also the mid-tier API, which we&#39;ll see later in the book). As an example, let&#39;s consider the problem of image regression. This refers to learning from a dataset where the independent variable is an image, and the dependent variable is one or more floats. Often we see people treat image regression as a whole separate application—but as you&#39;ll see here, we can treat it as just another CNN on top of the data block API. . We&#39;re going to jump straight to a somewhat tricky variant of image regression, because we know you&#39;re ready for it! We&#39;re going to do a key point model. A key point refers to a specific location represented in an image—in this case, we&#39;ll use images of people and we&#39;ll be looking for the center of the person&#39;s face in each image. That means we&#39;ll actually be predicting two values for each image: the row and column of the face center. . Assemble the Data . We will use the Biwi Kinect Head Pose dataset for this section. We&#39;ll begin by downloading the dataset as usual: . path = untar_data(URLs.BIWI_HEAD_POSE) . Let&#39;s see what we&#39;ve got! . path.ls().sorted() . (#50) [Path(&#39;01&#39;),Path(&#39;01.obj&#39;),Path(&#39;02&#39;),Path(&#39;02.obj&#39;),Path(&#39;03&#39;),Path(&#39;03.obj&#39;),Path(&#39;04&#39;),Path(&#39;04.obj&#39;),Path(&#39;05&#39;),Path(&#39;05.obj&#39;)...] . There are 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding .obj file for each (we won&#39;t need them here). Let&#39;s take a look inside one of these directories: . (path/&#39;01&#39;).ls().sorted() . (#1000) [Path(&#39;01/depth.cal&#39;),Path(&#39;01/frame_00003_pose.txt&#39;),Path(&#39;01/frame_00003_rgb.jpg&#39;),Path(&#39;01/frame_00004_pose.txt&#39;),Path(&#39;01/frame_00004_rgb.jpg&#39;),Path(&#39;01/frame_00005_pose.txt&#39;),Path(&#39;01/frame_00005_rgb.jpg&#39;),Path(&#39;01/frame_00006_pose.txt&#39;),Path(&#39;01/frame_00006_rgb.jpg&#39;),Path(&#39;01/frame_00007_pose.txt&#39;)...] . Inside the subdirectories, we have different frames, each of them come with an image (_rgb.jpg) and a pose file (_pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that converts an image filename to its associated pose file: . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) . Path(&#39;13/frame_00349_pose.txt&#39;) . Let&#39;s take a look at our first image: . im = PILImage.create(img_files[0]) im.shape . (480, 640) . im.to_thumb(160) . The Biwi dataset website used to explain the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren&#39;t important for our purposes, so we&#39;ll just show the function we use to extract the head center point: . cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) . This function returns the coordinates as a tensor of two items: . get_ctr(img_files[0]) . tensor([384.6370, 259.4787]) . We can pass this function to DataBlock as get_y, since it is responsible for labeling each item. We&#39;ll resize the images to half their input size, just to speed up training a bit. . One important point to note is that we should not just use a random splitter. The reason for this is that the same people appear in multiple images in this dataset, but we want to ensure that our model can generalize to people that it hasn&#39;t seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person&#39;s images. . The only other difference from the previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images: . biwi = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) . . Important: Points and Data Augmentation: We&#8217;re not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So, if you&#8217;re working with another library, you may need to disable data augmentation for these kinds of problems. . Before doing any modeling, we should look at our data to confirm it seems okay: . dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) . That&#39;s looking good! As well as looking at the batch visually, it&#39;s a good idea to also look at the underlying tensors (especially as a student; it will help clarify your understanding of what your model is really seeing): . xb,yb = dls.one_batch() xb.shape,yb.shape . (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) . Make sure that you understand why these are the shapes for our mini-batches. . Here&#39;s an example of one row from the dependent variable: . yb[0] . TensorPoint([[-0.3375, 0.2193]], device=&#39;cuda:6&#39;) . As you can see, we haven&#39;t had to use a separate image regression application; all we&#39;ve had to do is label the data, and tell fastai what kinds of data the independent and dependent variables represent. . It&#39;s the same for creating our Learner. We will use the same function as before, with one new parameter, and we will be ready to train our model. . Training a Model . As usual, we can use cnn_learner to create our Learner. Remember way back in &lt;&gt; how we used y_range to tell fastai the range of our targets? We&#39;ll do the same here (coordinates in fastai and PyTorch are always rescaled between -1 and +1):&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = cnn_learner(dls, resnet18, y_range=(-1,1)) . y_range is implemented in fastai using sigmoid_range, which is defined as: . def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo . This is set as the final layer of the model, if y_range is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range (lo,hi). . Here&#39;s what it looks like: . plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) . /home/jhoward/anaconda3/lib/python3.7/site-packages/fastbook/__init__.py:55: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . We didn&#39;t specify a loss function, which means we&#39;re getting whatever fastai chooses as the default. Let&#39;s see what it picked for us: . dls.loss_func . FlattenedLoss of MSELoss() . This makes sense, since when coordinates are used as the dependent variable, most of the time we&#39;re likely to be trying to predict something as close as possible; that&#39;s basically what MSELoss (mean squared error loss) does. If you want to use a different loss function, you can pass it to cnn_learner using the loss_func parameter. . Note also that we didn&#39;t specify any metrics. That&#39;s because the MSE is already a useful metric for this task (although it&#39;s probably more interpretable after we take the square root). . We can pick a good learning rate with the learning rate finder: . learn.lr_find() . SuggestedLRs(lr_min=0.005754399299621582, lr_steep=0.033113110810518265) . We&#39;ll try an LR of 1e-2: . lr = 1e-2 learn.fine_tune(3, lr) . epoch train_loss valid_loss time . 0 | 0.049630 | 0.007602 | 00:42 | . epoch train_loss valid_loss time . 0 | 0.008714 | 0.004291 | 00:53 | . 1 | 0.003213 | 0.000715 | 00:53 | . 2 | 0.001482 | 0.000036 | 00:53 | . Generally when we run this we get a loss of around 0.0001, which corresponds to an average coordinate prediction error of: . math.sqrt(0.0001) . 0.01 . This sounds very accurate! But it&#39;s important to take a look at our results with Learner.show_results. The left side are the actual (ground truth) coordinates and the right side are our model&#39;s predictions: . learn.show_results(ds_idx=1, nrows=3, figsize=(6,8)) . It&#39;s quite amazing that with just a few minutes of computation we&#39;ve created such an accurate key points model, and without any special domain-specific application. This is the power of building on flexible APIs, and using transfer learning! It&#39;s particularly striking that we&#39;ve been able to use transfer learning so effectively even between totally different tasks; our pretrained model was trained to do image classification, and we fine-tuned for image regression. . Conclusion . In problems that are at first glance completely different (single-label classification, multi-label classification, and regression), we end up using the same model with just different numbers of outputs. The loss function is the one thing that changes, which is why it&#39;s important to double-check that you are using the right loss function for your problem. . fastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want: . nn.CrossEntropyLoss for single-label classification | nn.BCEWithLogitsLoss for multi-label classification | nn.MSELoss for regression | . Questionnaire . How could multi-label classification improve the usability of the bear classifier? | How do we encode the dependent variable in a multi-label classification problem? | How do you access the rows and columns of a DataFrame as if it was a matrix? | How do you get a column by name from a DataFrame? | What is the difference between a Dataset and DataLoader? | What does a Datasets object normally contain? | What does a DataLoaders object normally contain? | What does lambda do in Python? | What are the methods to customize how the independent and dependent variables are created with the data block API? | Why is softmax not an appropriate output activation function when using a one hot encoded target? | Why is nll_loss not an appropriate loss function when using a one-hot-encoded target? | What is the difference between nn.BCELoss and nn.BCEWithLogitsLoss? | Why can&#39;t we use regular accuracy in a multi-label problem? | When is it okay to tune a hyperparameter on the validation set? | How is y_range implemented in fastai? (See if you can implement it yourself and test it without peeking!) | What is a regression problem? What loss function should you use for such a problem? | What do you need to do to make sure the fastai library applies the same data augmentation to your input images and your target point coordinates? | Further Research . Read a tutorial about Pandas DataFrames and experiment with a few methods that look interesting to you. See the book&#39;s website for recommended tutorials. | Retrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don&#39;t contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single-label dataset is impacted using multi-label classification. | &lt;/div&gt; .",
            "url": "https://pranath.github.io/fastbook/2021/02/19/06_multicat.html",
            "relUrl": "/2021/02/19/06_multicat.html",
            "date": " • Feb 19, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pranath.github.io/fastbook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pranath.github.io/fastbook/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}